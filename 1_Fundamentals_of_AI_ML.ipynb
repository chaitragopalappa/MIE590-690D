{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGgZnLflWNsqWP7P5gDjdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/1_Fundamentals_of_AI_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamentals of AI/ML - A review of course pre-requisites\n",
        "\n",
        "### Expected learning outcomes\n",
        "*   Overview of Artificial intelligence (AI), Machine Learning (ML), and Deep Learning (DL); and the taxonomy of topics\n",
        "*   Refresher on course prereuisites  \n",
        "  * Understand the objective of AI/ML\n",
        "  *   Refresher on math foundations that are prerequisistes for this course\n",
        "  *   Refresher on statistical ML (prerequisite for this course)\n",
        "  *   Understand the focus of this course - Deep Learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JbMYBR3ZFT3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Intelligence (AI) v. Machine Learning (ML) v.  Deep Learning (DL)"
      ],
      "metadata": {
        "id": "i4cZrpw-KzW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the objective of a AI/ML model?\n",
        "*   Fit a model to a given a dataset (i.e., learn the mathematical form of the data) to achieve a specific task\n",
        "*   Tasks could range from prediction, classificaton, clustering, to generative\n",
        "*   Multiple methods to achive this objective\n",
        "\n",
        "\n",
        "<table style=\"font-family: Arial, sans-serif; font-size: 20px;\">\n",
        "  <tr>\n",
        "    <th>ML category →</th>\n",
        "    <th>Supervised</th>\n",
        "    <th>Unsupervised/Semi-supervised</th>\n",
        "    <th>Reinforcement learning</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Tasks →</strong></td>\n",
        "    <td>Prediction, Classification, Inference, Generative</td>\n",
        "    <td>Clustering, Dimensionality reduction, Generative</td>\n",
        "    <td>Actions (decisions)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Model-based or Statistical ML →</strong></td>\n",
        "    <td>\n",
        "      Decision trees, Linear/logistic regression, Decision trees <br>\n",
        "      <span style=\"color:#b8860b;\"><strong>(MIE 522/622)</strong></span>\n",
        "    </td>\n",
        "    <td>\n",
        "      K-means, PCA <br>\n",
        "      <span style=\"color:#b8860b;\"><strong>(MIE 522/622)</strong></span>\n",
        "    </td>\n",
        "    <td><span style=\"color:red;\"><strong>MIE 524/624</strong></span></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Deep Learning building blocks →</strong></td>\n",
        "    <td colspan=\"3\">\n",
        "      Feedforward linear or non-linear layers, convolution layers, recurrent layers, attention layers\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Deep Learning (Model-free) methods →</strong></td>\n",
        "    <td>\n",
        "      FNN, CNN, RNN, Transformers, GNN <br>\n",
        "      <span style=\"color:green;\"><strong>(MIE 590/690D)</strong></span>\n",
        "    </td>\n",
        "    <td>\n",
        "      Autoencoders <br>\n",
        "      <span style=\"color:green;\"><strong>(MIE 590/690D)</strong></span>\n",
        "    </td>\n",
        "    <td><span style=\"color:red;\"><strong>MIE 524/624</strong></span></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Generative AI</strong></td>\n",
        "    <td colspan=\"3\">\n",
        "      LLM (BERT-encoder only transformer; GPT, LLaMA, DeepSeek-decoder only transformer)\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "-wrgpq2a3Ak2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised v. unsupervised v. reinforcement learning\n",
        "* Supervised learning uses labeled data to train models\n",
        "* Unsupervised learning uses unlabeled data to find patterns and relationships\n",
        "* Reinforcement learning involves training agents to make decisions\n",
        "\n"
      ],
      "metadata": {
        "id": "M6NHCldHXDg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical foundations of ML"
      ],
      "metadata": {
        "id": "DnZ8AoHQUgDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Math foundations](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=o8Ogoo6qE1gh)\n",
        "* Random variables\n",
        "  * Discrete RV\n",
        "  * Continuous RV\n",
        "  * Sigmoid (logistic) function\n",
        "  * Softmax function\n",
        "* Linear algebra\n",
        "  * Vectors spaces\n",
        "  * Norms of vector and matrix\n",
        "  * Matrix multiplication\n",
        "  * Matrix inversion\n",
        "* Matrix calculus\n",
        "  * Derivatives\n",
        "  * Gradients\n",
        "  * Jacobian\n",
        "  * Hessian\n",
        "* Optimization\n",
        "  * First-order methods (descent direction, stepsize, convergence rate)\n",
        "  * Stochastic gradient descent\n",
        "\n"
      ],
      "metadata": {
        "id": "USeu-6qTUpCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Model Based) Statistical ML Overview\n",
        "These are topics typically covered under [Core I of AI Engineering certificate](https://www.umass.edu/engineering/academics/ai-engineering-graduate-certificate/curriculum-ai-graduate-certificate).\n",
        "This course is Core 2 of AI Engineering certificate. This section is an overview of core concepts from Core 1."
      ],
      "metadata": {
        "id": "rTu4LZp9ir7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Model Based) Statistical ML Overview\n",
        "Supervised learning: Given data $(x,y)$, fit a model of the form $y = f(x)$ or $p(y|x)$\n",
        "*  They are generally model-based or model-driven techniques unlike deep learning that are data-driven, i.e., learn patterns through use of large data (note: deep learning is not math-free it is model-free)\n",
        "*  They include methods that are inherently interpretable to generally more interpretable than deep learning\n",
        "*  Commonly used methods\n",
        "  * [Linear regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "  * [Polynomial regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "  * [Logistic regression for prediction](https://developers.google.com/machine-learning/crash-course/logistic-regression)\n",
        "  *   [Decision trees for classification](https://scikit-learn.org/stable/modules/tree.html)\n",
        "\n",
        "  *   [Support vector machines for classification](https://scikit-learn.org/stable/modules/svm.html)\n",
        "  * [Nearest neighbors](https://scikit-learn.org/stable/modules/neighbors.html)\n",
        "  *   [K-means for clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
        "  * [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
        "  * [Dimensionality reduction using PCA](https://scikit-learn.org/stable/modules/unsupervised_reduction.html#)\n",
        "\n",
        "\n",
        "**Next few sections outline key concepts using terminologies typical in Deep Learning to help connect the two areas and understand differences**\n",
        "\n"
      ],
      "metadata": {
        "id": "HXWFJOuqFruN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Eq$\n",
        "* [Linear regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "    * [Model architecture](https://developers.google.com/machine-learning/crash-course/linear-regression): linear equation\n",
        "    $\\hat{y} = b+ w_1 x_1 + w_2 x_2 + w_3 x_3 + ... + w_N x_N$\n",
        "    * [Model fitting or Objective function](https://developers.google.com/machine-learning/crash-course/linear-regression/loss): minimize \"Loss\" between actual values ($y$) and predicted values ($\\hat{y}$)\n",
        "    * [Optimizer(solution method)](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=ZbhdrHKpaeAl) or [Link 2](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent): stochastic gradient descent\n",
        "    * [Regularization techniques (to avoid overfitting)](https://aunnnn.github.io/ml-tutorial/html/blog_content/linear_regression/linear_regression_regularized.html): Ridge, Lasso, Elastic Net\n",
        "    * [Analysis steps with programming- in TensorFlow](https://www.tensorflow.org/guide/core/quickstart_core)\n",
        "    * [Analysis steps with programming guide](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_taxi.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=linear_regression#scrollTo=W6a7dtcCob-n)"
      ],
      "metadata": {
        "id": "WpDsDAAp4RnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        " * [Polynomial regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "    * [Model architecture](https://developers.google.com/machine-learning/crash-course/linear-regression): Assume a suitable polynomial form to add non-linearity to the model (feature preprocessing or feature engineering), e.g.,  $\\hat{y} = b+ w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_1 x_2$\n",
        "    * Transform polynomial to linear equation\n",
        "      $\\hat{y} = b + w_1 u_1 + w_2 u_2 + w_3 u_3 + w_4 u_4;$ where $u_1 =x_1; u_2 = x_2; u_3 = x_1^2; u_4 = x_1 x_2 $\n",
        "    * Everything else follows linear regression"
      ],
      "metadata": {
        "id": "04JTdwlf4hb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Eq$\n",
        "* [Logistic regression for prediction](https://developers.google.com/machine-learning/crash-course/logistic-regression)\n",
        "    * [Model architecture](https://developers.google.com/machine-learning/crash-course/logistic-regression/sigmoid-function): polynomial transformed to linear and passed through sigmoid function;  $\\hat{y} = \\frac{1}{1+e^{-z}}; z = b+ w_1 x_1 + w_2 x_2 + w_3 x_3; $\n",
        "    * Model fitting or Objective function: minimize \"Log Loss\" between actual and predicted\n",
        "    * Optimizer: stochastic gradient descent and its variants\n",
        "    * [Analysis steps with programming in TensorFlow](https://www.tensorflow.org/guide/core/logistic_regression_core)"
      ],
      "metadata": {
        "id": "5PGDocQT4oCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        " *   [Decision trees for classification](https://scikit-learn.org/stable/modules/tree.html)\n",
        "    * Model architecture [Chapter 18, PML: An Introduction by Murphy](https://probml.github.io/pml-book/book1.html): $\\hat{y}  = f(x; θ) = \\sum_{j=1}^{J}w_j\\mathbb{I} (x ∈ R_j)$, where,   \n",
        "    $R_j$ is the region specified by the $j$’th leaf node,   \n",
        "    $w_j$ is the predicted output for that node given by $w_j = \\frac{\\sum_{n=1}^{n}y_n\\mathbb{I} (x_n ∈ R_j)}{\\sum_{n=1}^{n}\\mathbb{I} (x_n ∈ R_j)}$,   $θ = {(R_j ,w_j) : j = 1 : J}$, and   \n",
        "    $J$ is the number of nodes\n",
        "    * Model fitting or Objective function: minimize \"Loss\" between actual and predicted\n",
        "    * Optimizer or solution method: greedy procedure (gradient descent and variants cannot be used beause objective functions is not differentialble (discrete tree structure)) [Chapter 18, PML: An Introduction by Murphy](https://probml.github.io/pml-book/book1.html)"
      ],
      "metadata": {
        "id": "g2Noba9D4pJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "5N8kWjBYngl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing overview\n",
        "[Google ML crash course on Data](https://developers.google.com/machine-learning/crash-course/numerical-data) :\n",
        "1. Working with numerical data\n",
        "2. Working with categorical data\n",
        "3. Datasets, generalization and overfitting\n",
        "1. Vizualize the data: review data statistics,check for outliners, check for bad data\n",
        "2. Data preparation: normalize (min-max scaling, z-score normalization, log transformation, clipping)\n"
      ],
      "metadata": {
        "id": "S7jRbTadnoyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges with model-based ML"
      ],
      "metadata": {
        "id": "9gq58BbId-os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges with model-based ML\n",
        "Not suitable for complex non-linear functions, where linear or manual feature engineering (polynomial forms) are not a sufficient approximation  \n",
        "Deep learning: does non-linear feature extraction automatically\n",
        "  * Feed forward neural network (FFNN) or MLP\n",
        "  * CNN\n",
        "  * RNN"
      ],
      "metadata": {
        "id": "6yr8DnjyeELO"
      }
    }
  ]
}