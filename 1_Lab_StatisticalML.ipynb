{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvxylnm4wUws6t3/EJjLUb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/1_Lab_StatisticalML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: Statistical ML\n",
        "Using linear regression as the sample model, we will use this first lab as warm-up on:\n",
        "* coding,\n",
        "* data handling,\n",
        "* SGD optimizer\n",
        "* model fitting - data splitting for train /test; data batching\n",
        "* regularization"
      ],
      "metadata": {
        "id": "yb13ZQe32pSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review**\n",
        "$Eq$\n",
        "\n",
        "* [Overview of Linear regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "    * [Model architecture](https://developers.google.com/machine-learning/crash-course/linear-regression): linear equation\n",
        "    $\\hat{y} = b+ w_1 x_1 + w_2 x_2 + w_3 x_3 + ... + w_N x_N$\n",
        "    * [Model fitting or Objective function](https://developers.google.com/machine-learning/crash-course/linear-regression/loss): minimize \"Loss\" between actual values ($y$) and predicted values ($\\hat{y}$)\n",
        "    $$min_{\\mathbf{w}}L(\\mathbf{w})$$\n",
        "\n",
        "    $$ L(\\mathbf{w}) = \\sum_{i=1}^n (y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2$$\n",
        "\n",
        "    * [Optimizer(solution method)](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=ZbhdrHKpaeAl) or [Link 2](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent): stochastic gradient descent\n",
        "\n"
      ],
      "metadata": {
        "id": "GdP5RQqe2aOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review**\n",
        "* Data processing overview\n",
        "[Google ML crash course on Data](https://developers.google.com/machine-learning/crash-course/numerical-data) :\n",
        "  1. Vizualize the data: review data statistics,check for outliners, check for bad data\n",
        "  * [Vizualize the data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numerical_data_stats.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=numerical_data_stats#scrollTo=HYn5jBq2_Onh)\n",
        "  * [Modify bad data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numerical_data_bad_values.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=numerical_data_bad_values#scrollTo=RlPvUoDDsLoT)\n",
        "  2. [Data preparation](https://developers.google.com/machine-learning/crash-course/numerical-data/normalization): normalize (min-max scaling, z-score normalization, log transformation, clipping)"
      ],
      "metadata": {
        "id": "qgwwNQUC2dc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review**\n",
        "* [Stochastic gradient descent Optimizer](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=ZbhdrHKpaeAl) or [Link 2](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent)\n",
        "  * Review the SGD algorithm\n",
        "  * Review data batching"
      ],
      "metadata": {
        "id": "k7L_fMlH-slC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review codes**\n",
        "* [Predict fuel efficiency - TensorFlow](https://www.tensorflow.org/guide/core/quickstart_core)\n",
        "  * Not using inbuilt functions for regression model or SDG  \n",
        "* [Linear regression_Taxi_Analysis steps with programming guide](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_taxi.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=linear_regression#scrollTo=W6a7dtcCob-n)\n",
        "  * This has more detailed data vizualization and processing, and hyperparameter tuning exercises. It doe not use inbuilt packages either.\n"
      ],
      "metadata": {
        "id": "yeYsn7de6SZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review**\n",
        "* [Regularization techniques (to avoid overfitting)](https://aunnnn.github.io/ml-tutorial/html/blog_content/linear_regression/linear_regression_regularized.html): Ridge, Lasso, Elastic Net"
      ],
      "metadata": {
        "id": "pQRAy5xfIEED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Programming exercise**\n",
        "* Modify the [Predict fuel efficiency - TensorFlow](https://www.tensorflow.org/guide/core/quickstart_core) code to add regularizations"
      ],
      "metadata": {
        "id": "8LW_mOdSLs-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FYI- Statistical package**\n",
        "* Scikit Learn is the package typially used for statistical machine learning. It has inbuilt regression functions\n",
        "* [Sample linear regression  code](https://scikit-learn.org/1.5/auto_examples/linear_model/plot_ols.html)\n",
        "\n",
        "Deep Learning packages\n",
        "* Keras, Pytorch, Tensorflow are used for deep learing"
      ],
      "metadata": {
        "id": "bRXcv4mqMJD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## AI Generated\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 1. Linear Regression without Regularization\n",
        "print(\"--- Linear Regression (No Regularization) ---\")\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_scaled, y_train)\n",
        "y_pred_lin_reg = lin_reg.predict(X_test_scaled)\n",
        "rmse_lin_reg = np.sqrt(mean_squared_error(y_test, y_pred_lin_reg))\n",
        "print(f\"RMSE: {rmse_lin_reg:.2f}\")\n",
        "print(f\"Coefficients: {lin_reg.coef_}\")\n",
        "\n",
        "# 2. Lasso Regression (L1 Regularization)\n",
        "print(\"\\n--- Lasso Regression (L1 Regularization) ---\")\n",
        "# alpha is the regularization strength; higher alpha means stronger regularization\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X_train_scaled, y_train)\n",
        "y_pred_lasso = lasso_reg.predict(X_test_scaled)\n",
        "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
        "print(f\"RMSE: {rmse_lasso:.2f}\")\n",
        "print(f\"Coefficients: {lasso_reg.coef_}\")\n",
        "\n",
        "# 3. Ridge Regression (L2 Regularization)\n",
        "print(\"\\n--- Ridge Regression (L2 Regularization) ---\")\n",
        "# alpha is the regularization strength; higher alpha means stronger regularization\n",
        "ridge_reg = Ridge(alpha=1.0)\n",
        "ridge_reg.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge = ridge_reg.predict(X_test_scaled)\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "print(f\"RMSE: {rmse_ridge:.2f}\")\n",
        "print(f\"Coefficients: {ridge_reg.coef_}\")"
      ],
      "metadata": {
        "id": "W4FKmpgQNgRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}