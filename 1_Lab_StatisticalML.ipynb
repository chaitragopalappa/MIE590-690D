{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgcmRHPHcULL0FZImFcG3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/1_Lab_StatisticalML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Class Policy for Lab**\n",
        "* **Use of AI for 'Review' sections**\n",
        "  * YOU CAN use it to understand concepts and sections of the code that are new to you\n",
        "* **Use of AI for sections 'Questions' and 'Programming exercises'**\n",
        "  * YOU CAN use to it verify your responses, or find more efficient ways of coding\n",
        "  * DO NOT use it to directly find answers. If you have done a good 'review' you should be able to answer these questions\n",
        "* **Discussions with classmates**\n",
        "  * YOU CAN 'discuss' with your classmates\n",
        "  * DO NOT 'copy' from classmates\n",
        "\n",
        "* **GRADING**: 10% of overall grades are for participation during lab time.\n",
        "  * There is no submisison, attend and participate in the lab assignment for that day.\n",
        "  * I will individually interact with you to extent possible.\n",
        "  * We may discuss some general responses as a class.\n",
        "  * I will collectively use these.\n",
        "  * If you have to miss a class let me know in advance if possible, or as soon as feasible."
      ],
      "metadata": {
        "id": "rQn5bje3Pbg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: Statistical ML\n",
        "Using linear regression as the sample model, we will use this first lab as warm-up on:\n",
        "* coding,\n",
        "* data handling,\n",
        "* SGD optimizer\n",
        "* model fitting - data splitting for train /test; data batching\n",
        "* regularization"
      ],
      "metadata": {
        "id": "yb13ZQe32pSN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hw4IyifjPflq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review**\n",
        "$Eq$\n",
        "\n",
        "* [Overview of Linear regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "    * [Model architecture](https://developers.google.com/machine-learning/crash-course/linear-regression): linear equation\n",
        "    $\\hat{y} = b+ w_1 x_1 + w_2 x_2 + w_3 x_3 + ... + w_N x_N$\n",
        "    * [Model fitting or Objective function](https://developers.google.com/machine-learning/crash-course/linear-regression/loss): minimize \"Loss\" between actual values ($y$) and predicted values ($\\hat{y}$) using $n$ datapoints\n",
        "    $$min_{\\mathbf{w}}L(\\mathbf{w})$$\n",
        "\n",
        "    $$ L(\\mathbf{w}) = \\sum_{i=1}^n (y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2$$\n",
        "\n",
        "    * [Optimizer(solution method)](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=ZbhdrHKpaeAl) or [Link 2](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent): stochastic gradient descent\n",
        "\n"
      ],
      "metadata": {
        "id": "GdP5RQqe2aOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review**\n",
        "* Data processing overview\n",
        "[Google ML crash course on Data](https://developers.google.com/machine-learning/crash-course/numerical-data) :\n",
        "  1. Vizualize the data: review data statistics,check for outliners, check for bad data\n",
        "  * [Vizualize the data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numerical_data_stats.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=numerical_data_stats#scrollTo=HYn5jBq2_Onh)\n",
        "  * [Modify bad data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numerical_data_bad_values.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=numerical_data_bad_values#scrollTo=-o-X5uIc7WJS)\n",
        "  2. [Data preparation](https://developers.google.com/machine-learning/crash-course/numerical-data/normalization): normalize (min-max scaling, z-score normalization, log transformation, clipping)"
      ],
      "metadata": {
        "id": "qgwwNQUC2dc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review**\n",
        "* [Stochastic gradient descent Optimizer](https://github.com/chaitragopalappa/MIE590-690D/blob/main/suppl_files/1suppl_Mathematical_foundations_of_ML.ipynb) or [Link 2](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent)\n",
        "  * Review the SGD algorithm\n",
        "  * Review data batching"
      ],
      "metadata": {
        "id": "k7L_fMlH-slC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review codes**\n",
        "* [Predict fuel efficiency - TensorFlow](https://www.tensorflow.org/guide/core/quickstart_core)\n",
        "  * Observe use of object oriented programming\n",
        "    * 'Normalize' is a class;'norm_x' and norm_y' are objects of class\n",
        "    * def __init__(self, x)  #initializes variables at object creation\n",
        "      ```python\n",
        "      ``\n",
        "      class Normalize(tf.Module):\n",
        "        def __init__(self, x):\n",
        "          # Initialize the mean and standard deviation for normalization\n",
        "          self.mean = tf.math.reduce_mean(x, axis=0)\n",
        "          self.std = tf.math.reduce_std(x, axis=0)\n",
        "\n",
        "        def norm(self, x):\n",
        "          # Normalize the input\n",
        "          return (x - self.mean)/self.std\n",
        "\n",
        "        def unnorm(self, x):\n",
        "          # Unnormalize the input\n",
        "          return (x * self.std) + self.mean\n",
        "      ```\n",
        "    * 'norm_x' and 'norm_y' are objects of class; and can access functions defined within a class  \n",
        "      ```python\n",
        "      norm_x = Normalize(x_train_ohe)\n",
        "      norm_y = Normalize(y_train)\n",
        "      x_train_norm, y_train_norm = norm_x.norm(x_train_ohe), norm_y.norm(y_train)\n",
        "      x_test_norm, y_test_norm = norm_x.norm(x_test_ohe), norm_y.norm(y_test)\n",
        "\n",
        "\n",
        "      ```\n",
        "\n",
        "   * Observe use of mini-batch training- batch size is typically $2^n$\n",
        "     * Observe of batching and shuffling\n",
        "        ```python\n",
        "        batch_size = 64\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train_norm))\n",
        "        train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0]).batch(batch_size)\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test_norm))\n",
        "        test_dataset = test_dataset.shuffle(buffer_size=x_test.shape[0]).batch(batch_size)\n",
        "        ```\n",
        "\n",
        "      * Experiment as needed, e.g., run this code to understand 'from_tensor_slices'\n",
        "      \n",
        "\n",
        "          ```python\n",
        "          #AI generated\n",
        "          import tensorflow as tf\n",
        "          import numpy as np\n",
        "\n",
        "          # Creating a dataset from a single NumPy array\n",
        "          data = np.array([1, 2, 3, 4, 5])\n",
        "          dataset_single = tf.data.Dataset.from_tensor_slices(data)\n",
        "\n",
        "          print(\"Dataset from single array:\")\n",
        "          for element in dataset_single:\n",
        "              print(element.numpy())\n",
        "\n",
        "          # Creating a dataset from paired features and labels\n",
        "          features = np.array([[10, 20], [30, 40], [50, 60]])\n",
        "          labels = np.array([0, 1, 0])\n",
        "          dataset_paired = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "\n",
        "          print(\"\\nDataset from paired features and labels:\")\n",
        "          for feature_slice, label_slice in dataset_paired:\n",
        "              print(f\"Features: {feature_slice.numpy()}, Label: {label_slice.numpy()}\")\n",
        "          ```\n",
        "  * Observe steps of Stochastic Gradient Descent (SDG)(we typically use an inbuilt Class functions to do SDG; this code helps to understand what is happening internally in those function calls)\n",
        "      ```python\n",
        "        # Iterate through the training data\n",
        "        for x_batch, y_batch in train_dataset:\n",
        "          with tf.GradientTape() as tape:\n",
        "            y_pred_batch = lin_reg(x_batch)\n",
        "            batch_loss = mse_loss(y_pred_batch, y_batch)\n",
        "          # Update parameters with respect to the gradient calculations\n",
        "          grads = tape.gradient(batch_loss, lin_reg.variables)\n",
        "          for g,v in zip(grads, lin_reg.variables):\n",
        "            v.assign_sub(learning_rate * g)\n",
        "          # Keep track of batch-level training performance\n",
        "          batch_losses_train.append(batch_loss)\n",
        "      ```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yeYsn7de6SZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**  \n",
        "**Below are the general steps of SDG algorithm:** (called after data processing and batching)\n",
        "1. Initialize weigths  (w); Set learning rate; Set T (number of epochs)\n",
        "2. Repeat for T epochs   \n",
        "   > Repeat for each mini-batch  \n",
        "   >> a.  Calculate gradients at the point w   \n",
        "   >> b.  Update w  \n",
        "  \n",
        "\n",
        "The last code snippet corresponds to step 2. **Can you decipher what is happening in code corresponding to 2a. and 2b.?**"
      ],
      "metadata": {
        "id": "5GZXYITpbmQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Programming exercise**\n",
        "* Modify the [Predict fuel efficiency - TensorFlow](https://www.tensorflow.org/guide/core/quickstart_core) code to add L1 and L2 regularization"
      ],
      "metadata": {
        "id": "8LW_mOdSLs-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review**\n",
        "* [Linear regression_Taxi_Analysis steps with programming guide](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_taxi.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=linear_regression#scrollTo=W6a7dtcCob-n)\n",
        "  * This has more detailed data vizualization and processing, and hyperparameter tuning exercises. It does not use inbuilt packages either.\n"
      ],
      "metadata": {
        "id": "zwMNvKlqXcxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FYI- Statistical package**\n",
        "* Scikit Learn is the package typically used for statistical machine learning. It has inbuilt regression functions\n",
        "* [Sample linear regression  code](https://scikit-learn.org/1.5/auto_examples/linear_model/plot_ols.html)\n",
        "\n",
        "Deep Learning packages\n",
        "* Keras, Pytorch, Tensorflow are used for deep learning. We will use object oriented programming (OOP) to create classes to define deep learning architectures (that inherits inbuilt classes). While Scikit learn are also built on OOP, we do not typically create our classes.  "
      ],
      "metadata": {
        "id": "bRXcv4mqMJD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## AI Generated\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 1. Linear Regression without Regularization\n",
        "print(\"--- Linear Regression (No Regularization) ---\")\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_scaled, y_train)\n",
        "y_pred_lin_reg = lin_reg.predict(X_test_scaled)\n",
        "rmse_lin_reg = np.sqrt(mean_squared_error(y_test, y_pred_lin_reg))\n",
        "print(f\"RMSE: {rmse_lin_reg:.2f}\")\n",
        "print(f\"Coefficients: {lin_reg.coef_}\")\n",
        "\n",
        "# 2. Lasso Regression (L1 Regularization)\n",
        "print(\"\\n--- Lasso Regression (L1 Regularization) ---\")\n",
        "# alpha is the regularization strength; higher alpha means stronger regularization\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X_train_scaled, y_train)\n",
        "y_pred_lasso = lasso_reg.predict(X_test_scaled)\n",
        "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
        "print(f\"RMSE: {rmse_lasso:.2f}\")\n",
        "print(f\"Coefficients: {lasso_reg.coef_}\")\n",
        "\n",
        "# 3. Ridge Regression (L2 Regularization)\n",
        "print(\"\\n--- Ridge Regression (L2 Regularization) ---\")\n",
        "# alpha is the regularization strength; higher alpha means stronger regularization\n",
        "ridge_reg = Ridge(alpha=1.0)\n",
        "ridge_reg.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge = ridge_reg.predict(X_test_scaled)\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "print(f\"RMSE: {rmse_ridge:.2f}\")\n",
        "print(f\"Coefficients: {ridge_reg.coef_}\")"
      ],
      "metadata": {
        "id": "W4FKmpgQNgRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba35631f-4217-41d3-f5f0-586fde8857d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Linear Regression (No Regularization) ---\n",
            "RMSE: 0.81\n",
            "Coefficients: [[1.63051407]]\n",
            "\n",
            "--- Lasso Regression (L1 Regularization) ---\n",
            "RMSE: 0.81\n",
            "Coefficients: [1.53051407]\n",
            "\n",
            "--- Ridge Regression (L2 Regularization) ---\n",
            "RMSE: 0.81\n",
            "Coefficients: [1.61038427]\n"
          ]
        }
      ]
    }
  ]
}