{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/kEDkd6DqZjGhaO5ba0RY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/1_Lecture_Fundamentals_of_AI_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamentals of AI/ML - A review of course pre-requisites\n",
        "\n",
        "### Expected learning outcomes\n",
        "*   Overview of Artificial intelligence (AI), Machine Learning (ML), and Deep Learning (DL); and the taxonomy of topics\n",
        "*   Refresher on course prereuisites  \n",
        "  * Understand the objective of AI/ML\n",
        "  *   Refresher on math foundations that are prerequisistes for this course\n",
        "  *   Refresher on statistical ML (prerequisite for this course)\n",
        "  *   Understand the focus of this course - Deep Learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JbMYBR3ZFT3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Intelligence (AI) v. Machine Learning (ML) v.  Deep Learning (DL)"
      ],
      "metadata": {
        "id": "i4cZrpw-KzW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the objective of a AI/ML model?\n",
        "*   Fit a model to a given a dataset (i.e., learn the mathematical form of the data) to achieve a specific task\n",
        "*   Tasks could range from prediction, classificaton, clustering, to generative\n",
        "*   Multiple methods to achive this objective\n",
        "\n",
        "\n",
        "<table style=\"font-family: Arial, sans-serif; font-size: 20px;\">\n",
        "  <tr>\n",
        "    <th>ML category →</th>\n",
        "    <th>Supervised</th>\n",
        "    <th>Unsupervised/Semi-supervised</th>\n",
        "    <th>Reinforcement learning</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Tasks →</strong></td>\n",
        "    <td>Prediction, Classification, Inference, Generative</td>\n",
        "    <td>Clustering, Dimensionality reduction, Generative</td>\n",
        "    <td>Actions (decisions)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Model-based or Statistical ML →</strong></td>\n",
        "    <td>\n",
        "      Decision trees, Linear/logistic regression, Decision trees <br>\n",
        "      <span style=\"color:#b8860b;\"><strong>(MIE 522/622)</strong></span>\n",
        "    </td>\n",
        "    <td>\n",
        "      K-means, PCA <br>\n",
        "      <span style=\"color:#b8860b;\"><strong>(MIE 522/622)</strong></span>\n",
        "    </td>\n",
        "    <td><span style=\"color:red;\"><strong>MIE 524/624</strong></span></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Deep Learning building blocks →</strong></td>\n",
        "    <td colspan=\"3\">\n",
        "      Feedforward linear or non-linear layers, convolution layers, recurrent layers, attention layers\n",
        "    </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Deep Learning (Model-free) methods →</strong></td>\n",
        "    <td>\n",
        "      FNN, CNN, RNN, Transformers, GNN <br>\n",
        "      <span style=\"color:green;\"><strong>(MIE 590/690D)</strong></span>\n",
        "    </td>\n",
        "    <td>\n",
        "      Autoencoders <br>\n",
        "      <span style=\"color:green;\"><strong>(MIE 590/690D)</strong></span>\n",
        "    </td>\n",
        "    <td><span style=\"color:red;\"><strong>MIE 524/624</strong></span></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><strong>Generative AI</strong></td>\n",
        "    <td colspan=\"3\">\n",
        "      LLM (BERT-encoder only transformer; GPT, LLaMA, DeepSeek-decoder only transformer)\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "-wrgpq2a3Ak2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised v. unsupervised v. reinforcement learning\n",
        "* Supervised learning uses labeled data to train models\n",
        "* Unsupervised learning uses unlabeled data to find patterns and relationships\n",
        "* Reinforcement learning involves training agents to make decisions\n",
        "\n"
      ],
      "metadata": {
        "id": "M6NHCldHXDg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical foundations of ML"
      ],
      "metadata": {
        "id": "DnZ8AoHQUgDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Math foundations](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=o8Ogoo6qE1gh)\n",
        "* Random variables\n",
        "  * Discrete RV\n",
        "  * Continuous RV\n",
        "  * Sigmoid (logistic) function\n",
        "  * Softmax function\n",
        "* Linear algebra\n",
        "  * Vectors spaces\n",
        "  * Norms of vector and matrix\n",
        "  * Matrix multiplication\n",
        "  * Matrix inversion\n",
        "* Matrix calculus\n",
        "  * Derivatives\n",
        "  * Gradients\n",
        "  * Jacobian\n",
        "  * Hessian\n",
        "* Optimization\n",
        "  * First-order methods (descent direction, stepsize, convergence rate)\n",
        "  * Stochastic gradient descent\n",
        "\n"
      ],
      "metadata": {
        "id": "USeu-6qTUpCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Model Based) Statistical ML Overview\n",
        "These are topics typically covered under [Core I of AI Engineering certificate](https://www.umass.edu/engineering/academics/ai-engineering-graduate-certificate/curriculum-ai-graduate-certificate).\n",
        "MIE 590/690D is Core 2 of AI Engineering certificate. This section is an overview of core concepts from Core 1."
      ],
      "metadata": {
        "id": "rTu4LZp9ir7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Model Based) Statistical ML Overview\n",
        "Supervised learning: Given data $(x,y)$, fit a model of the form $y = f(x)$ or $p(y|x)$\n",
        "*  They are generally model-based or model-driven techniques unlike deep learning that are data-driven, i.e., learn patterns through use of large data (note: deep learning is not math-free it is model-free)\n",
        "*  They include methods that are inherently interpretable to generally more interpretable than deep learning\n",
        "*  Commonly used methods\n",
        "    * [Linear regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "    * [Polynomial regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "    * [Logistic regression for prediction](https://developers.google.com/machine-learning/crash-course/logistic-regression)\n",
        "    *   [Decision trees for classification](https://scikit-learn.org/stable/modules/tree.html)\n",
        "\n",
        "    *   [Support vector machines for classification](https://scikit-learn.org/stable/modules/svm.html)\n",
        "    * [Nearest neighbors](https://scikit-learn.org/stable/modules/neighbors.html)\n",
        "    *   [K-means for clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
        "    * [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
        "    * [Dimensionality reduction using PCA](https://scikit-learn.org/stable/modules/unsupervised_reduction.html#)\n",
        "\n",
        "\n",
        "**Next few sections outline key concepts using terminologies typical in Deep Learning to help connect the two areas and understand differences**\n",
        "\n"
      ],
      "metadata": {
        "id": "HXWFJOuqFruN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Eq$\n",
        "* [Linear regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "    * [Model architecture](https://developers.google.com/machine-learning/crash-course/linear-regression): linear equation\n",
        "    $\\hat{y} = b+ w_1 x_1 + w_2 x_2 + w_3 x_3 + ... + w_N x_N$\n",
        "    * [Model fitting or Objective function](https://developers.google.com/machine-learning/crash-course/linear-regression/loss): minimize \"Loss\" between actual values ($y$) and predicted values ($\\hat{y}$)\n",
        "    $$min_{\\mathbf{w}}L(\\mathbf{w})$$\n",
        "\n",
        "    $$ L(\\mathbf{w}) = \\sum_{i=1}^n (y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2$$\n",
        "\n",
        "    * [Optimizer(solution method)](https://github.com/chaitragopalappa/MIE590-690D/blob/main/suppl_files/1suppl_Mathematical_foundations_of_ML.ipynb) or [Link 2](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent): stochastic gradient descent\n",
        "\n",
        "    * [Regularization techniques (to avoid overfitting)](https://aunnnn.github.io/ml-tutorial/html/blog_content/linear_regression/linear_regression_regularized.html): Ridge, Lasso, Elastic Net\n",
        "    * [Analysis steps with programming- in TensorFlow](https://www.tensorflow.org/guide/core/quickstart_core)\n",
        "    * [Linear regression_Taxi_Analysis steps with programming guide](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_taxi.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=linear_regression#scrollTo=W6a7dtcCob-n)"
      ],
      "metadata": {
        "id": "WpDsDAAp4RnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        " * [Polynomial regression for prediction]((https://developers.google.com/machine-learning/crash-course/linear-regression)\n",
        "    * [Model architecture](https://developers.google.com/machine-learning/crash-course/linear-regression): Assume a suitable polynomial form to add non-linearity to the model (feature preprocessing or feature engineering), e.g.,  $\\hat{y} = b+ w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_1 x_2$\n",
        "    * Transform polynomial to linear equation\n",
        "      $\\hat{y} = b + w_1 u_1 + w_2 u_2 + w_3 u_3 + w_4 u_4;$ where $u_1 =x_1; u_2 = x_2; u_3 = x_1^2; u_4 = x_1 x_2 $\n",
        "    * Everything else follows linear regression"
      ],
      "metadata": {
        "id": "04JTdwlf4hb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Eq$\n",
        "* [Logistic regression for prediction](https://developers.google.com/machine-learning/crash-course/logistic-regression)\n",
        "    * [Model architecture](https://developers.google.com/machine-learning/crash-course/logistic-regression/sigmoid-function): polynomial transformed to linear and passed through sigmoid function;  $\\hat{y} = \\frac{1}{1+e^{-z}}; z = b+ w_1 x_1 + w_2 x_2 + w_3 x_3; $\n",
        "    * Model fitting or Objective function: minimize negative log loss (binary cross-entropy function)between actual and predicted\n",
        "     $$min_{\\mathbf{w}}L(\\mathbf{w})$$\n",
        "    $$L(\\mathbf{w}) = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)]$$\n",
        "    * Optimizer: stochastic gradient descent and its variants\n",
        "    * [Analysis steps with programming in TensorFlow](https://www.tensorflow.org/guide/core/logistic_regression_core)\n",
        "\n"
      ],
      "metadata": {
        "id": "5PGDocQT4oCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        " *   [Decision trees for classification](https://scikit-learn.org/stable/modules/tree.html)\n",
        "\n",
        "* **Model architecture** [Chapter 18, PML: An Introduction by Murphy](https://probml.github.io/pml-book/book1.html):\n",
        "    \n",
        "    $\\hat{y}  = f(x; θ) = \\sum_{j=1}^{J}w_j\\mathbb{I} (x ∈ R_j)$, where,   \n",
        "  $R_j$ is the region specified by the $j$’th leaf node,   \n",
        "  $w_j$ is the predicted output for that node given by $w_j = \\frac{\\sum_{n=1}^{n}y_n\\mathbb{I} (x_n ∈ R_j)}{\\sum_{n=1}^{n}\\mathbb{I} (x_n ∈ R_j)}$,   $θ = {(R_j ,w_j) : j = 1 : J}$, and   \n",
        "  $J$ is the number of nodes\n",
        "\n",
        "  * **Model fitting or Objective function**: minimize \"Loss\" between actual and predicted\n",
        "  * **Optimizer or solution method**: greedy procedure (gradient descent and variants cannot be used beause objective functions is not differentialble (discrete tree structure)) [Chapter 18, PML: An Introduction by Murphy](https://probml.github.io/pml-book/book1.html)"
      ],
      "metadata": {
        "id": "g2Noba9D4pJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        "\n",
        "* [Support vector machines for classification](https://scikit-learn.org/stable/modules/svm.html)\n",
        "\n",
        "  * **Model architecture:** Linear SVM binary classifier\n",
        "  $$\n",
        "  h(x) = sign(f(x)) \\\\\n",
        "  f(x) = (\\mathbf{w}^\\top \\mathbf{x}_i + b)\n",
        "  $$\n",
        "\n",
        "  * Model fitting or **Objective function**:   Find a hyperplane that best separates two classes in feature space or maximize the margin (distance between the decision boundary and closest data points, i.e., support vectors).($ y_i$ are target labels +1 or -1)\n",
        "  $$\\max_{\\mathbf{w},b} \\, \\frac{1}{\\|\\mathbf{w}\\|^2} min_{i=1}^{N} y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) $$\n",
        "  Suppose we scale $\\mathbf{w}$ by $k$, i.e., $\\mathbf{w} \\rightarrow k\\mathbf{w}$ such that $y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b)$ = 1 for point closest to decision boundary. Then, we can rewrite objective function as\n",
        "  $$\n",
        "  \\min_{\\mathbf{w},b} \\, \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\\\ s.t. \\\\\n",
        "  y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1, \\; \\forall i\n",
        "  $$\n",
        "  * Optimizer **(solution method)**: Quadratic programming (QP) solvers or variants (e.g., SMO, SGD for large datasets)."
      ],
      "metadata": {
        "id": "HKS1m38UGilC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        "\n",
        "* [Nearest neighbors k-NN](https://scikit-learn.org/stable/modules/neighbors.html)\n",
        "\n",
        "  * **Model architecture:** Given a query point, find the $k$ closest training examples in feature space (typically by Euclidean distance). For classification, majority vote among neighbors; for regression, average target values.\n",
        "\n",
        "  * **Model fitting or Objective function:** No explicit training—instance-based learning; decision is made **at query time**.\n",
        "\n",
        "  * **Optimizer (solution method):** Efficient neighbor search (KD-tree, Ball tree, brute force for small/medium datasets)."
      ],
      "metadata": {
        "id": "9kDJbEM3JCAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        "\n",
        "* [K-means for clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
        "\n",
        "  * **Model architecture**: Partition $n$ observations into $k$ clusters $C_1, ..., C_k$ to minimize within-cluster sum of squares (WCSS):\n",
        "  $$\n",
        "  \\sum_{j=1}^{k} \\sum_{x_i \\in C_j} \\| x_i - \\mu_j \\|^2\n",
        "  $$\n",
        "\n",
        "     where $\\mu_j$ is the centroid of cluster $C_j$.\n",
        "  * Model fitting or **Objective function**: Minimize WCSS by updating cluster assignments and centroids iteratively.\n",
        "\n",
        "  * **Optimizer (solution method):** Expectation-Maximization-like alternating assignment/update (Lloyd’s algorithm)."
      ],
      "metadata": {
        "id": "L5tVIAG3JIw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        "\n",
        "* [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
        "\n",
        "  * **Model architecture:** Probabilistic classifier based on Bayes theorem, assumes conditional independence between every pair of features:\n",
        "\n",
        "  $$\n",
        "  p(y|x_1, x_2,....,x_n) \\propto p(y) \\prod_{i} p(x_i|y)\n",
        "  $$\n",
        "\n",
        "  * **Model fitting or Objective function:** Estimate class prior $p(y)$ and likelihood $p(x_i|y)$ from frequency/counts in data.\n",
        "\n",
        "  * **Optimizer (solution method):** Maximum likelihood estimation (MLE); closed-form parameter estimates."
      ],
      "metadata": {
        "id": "qY30CnOJJVmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $Eq$\n",
        "\n",
        "* [Dimensionality reduction using Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/unsupervised_reduction.html#)\n",
        "\n",
        "  * **Model architecture:** Linear transformation projecting data onto orthogonal directions of maximum variance (principal components). he basic idea is to find a linear and orthogonal projection of the high dimensional data $\\mathbf{x} \\in \\mathbb{R}^D$ to a low dimensional subspace $\\mathbf{z} \\in \\mathbb{R}^L$, such that the low dimensional representation is a “good approximation” to the original data. Specifically, if we encode by\n",
        "  $$\\mathbf{z} = W^\\top \\mathbf{x},$$ and decode by\n",
        "  $$ \\hat{\\mathbf{x}} = W \\mathbf{z},$$\n",
        "  then we want $\\hat{\\mathbf{x}}$ to be close to $\\mathbf{x}$ in $\\ell_2$ distance. The variables $\\mathbf{z}$ are a low dimensional representation of $\\mathbf{x}$, and thus known as the **latent variables**, as they consist of latent or “hidden” values that are not observed in the data.\n",
        "\n",
        "  * **Model fitting or Objective function:** Find orthogonal axes (principal components) that minimize reconstruction error (or distortion) ($\\mathcal{L}(W)$\n",
        "  $$\n",
        "  \\min_W \\mathcal{L}(W) \\\\\n",
        "  \\mathcal{L}(W) = \\sum_{n=1}^N \\left\\| \\mathbf{x}_n - \\hat{\\mathbf{x}}_n \\right\\|_2^2\n",
        "  = \\sum_{n=1}^N \\left\\| \\mathbf{x}_n - W W^\\top \\mathbf{x}_n \\right\\|_2^2.\n",
        "  $$\n",
        "  which is equivalent to maximizing variance of the projected data\n",
        "  $$\n",
        "  \\max_W \\operatorname{Tr}(W^\\top S W), \\quad W^\\top W = I\n",
        "  $$\n",
        "\n",
        "    where $S=\\frac{1}{N} X_c^\\top X_c  $ is the data covariance matrix on zero-centered data ($X_c=X-\\frac{1}{N}\\sum X$) .\n",
        "\n",
        "  * **Optimizer (solution method):** Eigen-decomposition or singular value decomposition (SVD) of covariance matrix."
      ],
      "metadata": {
        "id": "M6o8uklvJcUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "5N8kWjBYngl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing overview\n",
        "[Google ML crash course on Data](https://developers.google.com/machine-learning/crash-course/numerical-data) :\n",
        "1. Working with numerical data\n",
        "2. Working with categorical data\n",
        "3. Datasets, generalization and overfitting\n",
        "1. Vizualize the data: review data statistics,check for outliners, check for bad data\n",
        "2. Data preparation: normalize (min-max scaling, z-score normalization, log transformation, clipping)\n"
      ],
      "metadata": {
        "id": "S7jRbTadnoyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges with model-based ML"
      ],
      "metadata": {
        "id": "9gq58BbId-os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges with model-based ML\n",
        "Not suitable for complex non-linear functions, where linear or manual feature engineering of the non-linearities (e.g., polynomial regression) are not a sufficient approximation  \n",
        "Deep learning: learns or automatically extracts the non-linearity features in data\n",
        "  * Feed forward neural network (FFNN) or MLP\n",
        "  * CNN\n",
        "  * RNN"
      ],
      "metadata": {
        "id": "6yr8DnjyeELO"
      }
    }
  ]
}