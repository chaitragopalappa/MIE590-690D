{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2+eb0flLJhGnfJ9PMYhr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/2_Feed_forward_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13: Neural Networks for Structured Data or Tabular data\n",
        "(Feed Forward Neural Network)\n"
      ],
      "metadata": {
        "id": "-e1YX7UMzVku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outline\n",
        "* Objective function (minimize Loss)\n",
        "* Model architecture (multiple layers- 'deep'; each node in each layer is a linear transformation of previous layer passed through an activation function)\n",
        "* Optimizer (backprop that uses SGD and its variants in a Directed Acyclic Graph (DAG) structure)\n"
      ],
      "metadata": {
        "id": "FqGItbS75U0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "aEi3cyg_aBGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Obejctive function**\n",
        "Suppose ${\\mathbf{y}}=f(\\mathbf{{x}});\\mathbf{x}\\in \\mathbb{R}^n; \\mathbf{y}\\in\\mathbb{R}^m$  \n",
        "Objective function: $ Min\\mathcal{L}(\\mathbf{\\theta}) =Min_\\mathbf{\\theta}||\\mathbf{\\hat{y}-y}||$\n",
        "* $\\mathbf{\\hat{y}} $ are predicted values from neural network\n",
        "* $\\mathcal{L} $ is the Loss   \n",
        "* $\\mathbf{\\theta}=[\\mathbf{b},\\mathbf{W_1},\\mathbf{W_2},...,\\mathbf{W_L}]$ are neural network coefficients.\n"
      ],
      "metadata": {
        "id": "0WO7ndACPgZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NN Model Architecture**\n",
        "### Model architecture:\n",
        "$\\hat{y}  \\triangleq  z_L =f_L(z_{L−1})= f_L(f_{L-1}(z_{L−2}))=f_L(f_{L-1}(.....f_1(z_{0}))) ; z_0 \\triangleq \\mathbf{x}$  \n",
        "or  \n",
        "$\\hat{y}= f_L \\circ f_{L-1}\\circ f_{L-2}.....\\circ f_1$  \n",
        "where,   \n",
        "$\\mathbf{\\mathit{z}}_l = f_l(\\mathbf{\\mathit{z}}_{l−1}) = φ_l(\\mathbf{\\mathit{b}}_l +\\mathbf{W}_lz_{l−1})$  (Vector form)  \n",
        "\n",
        "> Rewriting in scalar form  \n",
        "$z_{kl} = φ_l (b_{kl} +\\sum_{j=1}^{K_{l-1}}{W}_{jkl}z_{jl−1}) $  \n",
        "The representation can be further simplified as   \n",
        "$z_{kl} = φ_l (\\sum_{j=0}^{K_{l-1}}{W}_{jkl}z_{jl−1}) $  by setting $z_{0l−1}=1$ and ${W}_{0kl}=b_{kl}$  \n",
        "\n",
        "Putting back in vector form:  \n",
        "$\\mathbf{\\mathit{z}}_l = φ_l (\\mathbf{W}_lz_{l−1}) $\n",
        "\n",
        "General architecture of FFNN:\n",
        "* has an input layer representing the features with dimensionality equal to the number of input features, i.e., $z_0 =\\mathbf{x}= [x_1, ...x_n]$;\n",
        "* has an output layer representing the predicted variable with dimentionaly equal to the output variable, i.e., $z_L =\\mathbf{\\hat{y}}$; $L$ is the last layer\n",
        "* has layers $l=1$ to $l=L-1$ as the hidden layers\n",
        "* each hidden layer $l$ has $K_l$ number of nodes\n",
        "* $z_{kl}$ is node $k$ in layer $l$\n",
        "* $\\mathbf{\\mathit{z}}_l$ is a vector of nodes at layer $l$\n",
        "* $b_{kl}$ is the bias node connecting to each node $k$ in layer $l$\n",
        "* $\\mathbf{W}_l$ is a matrix of coefficients; ${W}_{jkl}$ is coefficent from node $j$ in layer $l-1$ to node $k$ in layer $l$\n",
        "* hidden units $z_l$ at each layer $l$ is a linear transformation of the hidden units at the previous layer passed elementwise through an activation function.\n",
        "*$\\varphi$ is an activation function (could be any differntiable function to add non-linearity to the model; sigmoid, tanh, ReLU are commonly used)\n",
        "* it is typically a **fully connected** feed forward network (arrow from every node in layer $l-1$ to every node in layer $l$)\n"
      ],
      "metadata": {
        "id": "G-b5iTACzi6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "<td style=\"width:50%; vertical-align:top;\">\n",
        "\n",
        "## **NN Model Architecture**\n",
        "\n",
        "### Model architecture:\n",
        "$z_L =f_L(z_{L−1})= f_L(f_{L-1}(z_{L−2}))=f_L(f_{L-1}(\\dots f_1(z_{0}))) $  \n",
        "\n",
        ">$ \\mathbf{x} \\triangleq z_0  $  \n",
        "$\\hat{y}  \\triangleq  z_L$\n",
        "\n",
        "or  \n",
        "$\\hat{y}= f_L \\circ f_{L-1}\\circ f_{L-2} \\dots \\circ f_1$  \n",
        "\n",
        "where,  \n",
        "$\\mathbf{\\mathit{z}}_l = f_l(\\mathbf{\\mathit{z}}_{l−1}) = \\varphi_l(\\mathbf{\\mathit{b}}_l +\\mathbf{W}_l z_{l−1})$  (Vector form)  \n",
        "\n",
        "> Rewriting in scalar form  \n",
        "$z_{kl} = \\varphi_l (b_{kl} +\\sum_{j=1}^{K_{l-1}}{W}_{jkl}z_{jl−1})$  \n",
        "The representation can be further simplified as   \n",
        "$z_{kl} = \\varphi_l (\\sum_{j=0}^{K_{l-1}}{W}_{jkl}z_{jl−1})$  \n",
        "by setting $z_{0l−1}=1$ and ${W}_{0kl}=b_{kl}$  \n",
        "\n",
        "Putting back in vector form:  \n",
        "$\\mathbf{\\mathit{z}}_l = \\varphi_l (\\mathbf{W}_l z_{l−1})$\n",
        "\n",
        "</td>\n",
        "<td style=\"width:50%; vertical-align:top;\">\n",
        "\n",
        "### General architecture of FFNN\n",
        "\n",
        "* has an input layer representing the features with dimensionality equal to the number of input features,  \n",
        "  i.e., $z_0 =\\mathbf{x}= [x_1, \\dots, x_n]$\n",
        "* has an output layer representing the predicted variable with dimensionality equal to the output variable,  \n",
        "  i.e., $z_L =\\mathbf{\\hat{y}}$; $L$ is the last layer\n",
        "* has layers $l=1$ to $l=L-1$ as the hidden layers\n",
        "* each hidden layer $l$ has $K_l$ number of nodes\n",
        "* $z_{kl}$ is node $k$ in layer $l$\n",
        "* $\\mathbf{\\mathit{z}}_l$ is a vector of nodes at layer $l$\n",
        "* $b_{kl}$ is the bias node connecting to each node $k$ in layer $l$\n",
        "* $\\mathbf{W}_l$ is a matrix of coefficients; ${W}_{jkl}$ is coefficient from node $j$ in layer $l-1$ to node $k$ in layer $l$\n",
        "* hidden units $z_l$ at each layer $l$ are a linear transformation of the hidden units at the previous layer  \n",
        "  passed elementwise through an activation function\n",
        "* $\\varphi$ is an activation function (could be any differentiable function to add non-linearity; sigmoid, tanh, ReLU are common)\n",
        "* typically a **fully connected** feedforward network (arrow from every node in layer $l-1$ to every node in layer $l$)\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "MpEPyew6-JrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        " $\\mathbf{y}=f(\\mathbf{x});\\mathbf{x}\\in \\mathbb{R}^5; \\mathbf{y}\\in\\mathbb{R}^1$  where $\\sigma=  φ_l (b_{kl} +\\sum_{j=1}^{K_{l-1}}{W}_{jkl}z_{jl−1}) $ denoting the activiation function\n",
        " ![](https://raw.githubusercontent.com/chaitragopalappa/MIE590-690D/main/images/supply_chain_nn.png)\n",
        ""
      ],
      "metadata": {
        "id": "xbhg4x4T9qwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimizer**\n",
        "We can apply [stochastic gradient descent (SGD)](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=90ILVxtj9MRl)   \n",
        "$\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_t − η_t∇_\\mathbf{\\theta}\\mathcal{L}(\\mathbf{\\theta}_t)$  \n",
        "As discussed above\n",
        "* $\\mathbf{\\theta}=[\\mathbf{b},\\mathbf{W_1},\\mathbf{W_2},...,\\mathbf{W_L}]$ are the coefficients of the neural network, so we will have to apply SDG to each coefficient\n",
        "\n",
        "\n",
        "SGD application:\n",
        "* If we can estimate $∇_\\mathbf{\\theta}\\mathcal{L}(\\mathbf{\\theta}_t)$ we can apply SDG\n",
        "* How to estimate gradients of such complex function?\n",
        "  * $\\hat{y}= f_L \\circ f_{L-1}\\circ f_{L-2}.....\\circ f_1$\n",
        "  * $\\mathcal{L}= ||{\\hat{y}-y}||$ also written as $\\mathcal{L((\\mathbf{x},y),\\theta)}= ||{\\hat{y}-y}||$\n",
        "  * $\\mathcal{L} \\equiv f_L \\circ f_{L-1}\\circ f_{L-2}.....\\circ f_1$\n",
        "* Symbolic differentiation is tedious given the complex function (composition of functions)\n",
        "\n",
        "### Autodiff\n",
        " * Autodiff or automatic differentiation breaks a function into sequence of simple operators (that can be represented in a computational graph structure) and applies chain rule from calculus to sequentially calcuate the gradient of each operator\n",
        "  * The architecture of the neural network nuturally has a computational graph structure.\n",
        "  *  The sequence of derivative calculations can be forward mode or reverse mode.\n",
        "  * Given the nature of the NN architecture reverse model autodiff is more applicable here\n",
        "\n",
        "*The above full approach of using reverse mode autodiff with SGD is called BackProp (backpropogation) algorithm*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "82oEw9zKeP5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Autodiff for FFNN**\n",
        "\n",
        "Example: For a FFNN with a single hidden-layer the objective function is\n",
        " $\\mathcal{L((\\textbf{x},y),\\theta)}= ||{\\hat{y}-y}||= \\frac{1}{2}||y-\\mathbf{W}_2  φ_2 (\\mathbf{W}_l\\mathbf{x})||_2^2 $\n",
        "\n",
        "\n",
        " ### Feed forward pass of the NN calculates the value of $\\mathcal{L}$\n",
        " The objective function can be rewritten into sequence of 4 operators (4 layers from perspective of autodiff computational graph - do not confuse with NN architecture layers)\n",
        "\n",
        "*Figure embeded directly from [GitHub PML by Murphy](https://github.com/probml/pml-book/blob/main/book1-figures/Figure_13.10.png)*\n",
        " ![](https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_13.10.png)\n",
        "\n",
        "\n",
        "\n",
        "### Feed forward pass\n",
        "\n",
        " $\\mathcal{L}=f_4 \\circ f_3 \\circ f_2 \\circ f_1$    \n",
        " $\\mathbf{x}_2= f_1(\\mathbf{x},\\theta_1 )=\\mathbf{W}_1\\mathbf{x}$    \n",
        " $\\mathbf{x}_3= f_2(\\mathbf{x_2},{φ})={φ}(\\mathbf{x_2})$  \n",
        " $\\mathbf{x}_4= f_3(\\mathbf{x_3},\\theta_3 )=\\mathbf{W}_2\\mathbf{x}_3$  \n",
        " $\\mathcal{L} = f_4 (\\mathbf{x_4},y)=\\frac{1}{2}||y-x_4||_2^2 $\n",
        "\n",
        "### Reverse mode differentiation\n",
        "> $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_3} = \\frac{\\partial \\mathcal{L}}{\\partial x_4} \\frac{\\partial x_4}{\\partial \\theta_3}$  \n",
        "$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta_2}  = \\frac{\\partial \\mathcal{L}}{\\partial x_4} \\frac{\\partial x_4}{\\partial x_3} \\frac{\\partial x_3}{\\partial \\theta_2}$  \n",
        "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1}\n",
        "= \\frac{\\partial \\mathcal{L}}{\\partial x_4} \\frac{\\partial x_4}{\\partial x_3} \\frac{\\partial x_3}{\\partial x_2} \\frac{\\partial x_2}{\\partial \\theta_1}\n",
        "$\n",
        "\n"
      ],
      "metadata": {
        "id": "C6qKuXIup1B8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Backpropagation -general algorithm**\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\textbf{Algorithm: Backpropagation for an MLP with $K$ layers used inside SDG loop} \\\\[2mm]\n",
        "\\text{// Forward pass} \\\\\n",
        "1: \\quad x_1 := x \\\\\n",
        "2: \\quad \\text{for } k = 1 : K \\ \\text{do} \\\\\n",
        "3: \\quad\\quad x_{k+1} = f_k(x_k, \\theta_k) \\\\[2mm]\n",
        "\\text{// Backward pass} \\\\\n",
        "4: \\quad u_{K+1} := 1 \\\\\n",
        "5: \\quad \\text{for } k = K : 1 \\ \\text{do} \\\\\n",
        "6: \\quad\\quad g_k := u_{k+1}^\\top \\frac{\\partial f_k(x_k, \\theta_k)}{\\partial \\theta_k} \\\\\n",
        "7: \\quad\\quad u_k^\\top := u_{k+1}^\\top \\frac{\\partial f_k(x_k, \\theta_k)}{\\partial x_k} \\\\[2mm]\n",
        "\\text{// Output} \\\\\n",
        "8: Return \\quad \\mathcal{L} = x_{K+1}, \\quad \\nabla_x \\mathcal{L} = u_1, \\quad \\{\\nabla_{\\theta_k} \\mathcal{L} = g_k : k = 1 : K\\}\n",
        "\\end{array}\n",
        "$$"
      ],
      "metadata": {
        "id": "X-TYrvsh5RJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backprop example for FNNN with one hidden layer using Sigmoid activation\n",
        "[Slides](https://github.com/chaitragopalappa/MIE590-690D/blob/main/images/BackProp_Example.pdf)  \n",
        "[Code](https://github.com/chaitragopalappa/MIE590-690D/blob/main/Codes/NN_Backpropagation_Vizual.ipynb)"
      ],
      "metadata": {
        "id": "29XbQwe1McyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP for classifying 2D data into 2 categories"
      ],
      "metadata": {
        "id": "OlGpX_ONa37v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Tensor playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.02424&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
      ],
      "metadata": {
        "id": "tXnFYBptbKXG"
      }
    }
  ]
}