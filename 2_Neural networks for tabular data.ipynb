{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdxYB8TB3Ay5j/x3T7x+XM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter 13: Neural Networks for Structured Data or Tabular data\n","\n","## Feed Forward Neural Network (FFNN) / Multi-layer Perceptrons (MLP)\n","\n","\n","\n"],"metadata":{"id":"-e1YX7UMzVku"}},{"cell_type":"markdown","source":["### Outline\n","* Objective function (minimize Loss)\n","* Model architecture (multiple layers- 'deep'; each node in each layer is a linear transformation of previous layer passed through an activation function)\n","* Optimizer (backprop that uses SGD and its variants in a Directed Acyclic Graph (DAG) structure)\n"],"metadata":{"id":"FqGItbS75U0h"}},{"cell_type":"markdown","source":["# Multi-Layer Perceptron (MLP)\n","\n"],"metadata":{"id":"aEi3cyg_aBGc"}},{"cell_type":"markdown","source":["## **Objective function**\n","Suppose ${\\mathbf{y}}=f(\\mathbf{{x}});\\mathbf{x}\\in \\mathbb{R}^n; \\mathbf{y}\\in\\mathbb{R}^m$  \n","Objective function: $ Min\\mathcal{L}(\\mathbf{\\theta}) =Min_\\mathbf{\\theta}||\\mathbf{\\hat{y}-y}||$\n","* $\\mathbf{\\hat{y}} $ are predicted values from neural network\n","* $\\mathcal{L} $ is the Loss   \n","* $\\mathbf{\\theta}=[\\mathbf{b},\\mathbf{W_1},\\mathbf{W_2},...,\\mathbf{W_L}]$ are neural network coefficients.\n","\n","Thus, **'model fitting'** in context of NN refers to learning  $\\mathbf{\\theta}$ by minmizing the Loss function"],"metadata":{"id":"0WO7ndACPgZ-"}},{"cell_type":"markdown","source":["<table>\n","<tr>\n","<td style=\"width:50%; vertical-align:top;\">\n","\n","## **NN Model Architecture**\n","\n","### Model architecture:\n","$z_L =f_L(z_{L−1})= f_L(f_{L-1}(z_{L−2}))=f_L(f_{L-1}(\\dots f_1(z_{0}))) $  \n","\n",">$ \\mathbf{x} \\triangleq z_0  $  \n","$\\hat{y}  \\triangleq  z_L$\n","\n","or  \n","$\\hat{y}= f_L \\circ f_{L-1}\\circ f_{L-2} \\dots \\circ f_1$  \n","\n","where,  \n","$\\mathbf{\\mathit{z}}_l = f_l(\\mathbf{\\mathit{z}}_{l−1}) = \\varphi_l(\\mathbf{\\mathit{b}}_l +\\mathbf{W}_l z_{l−1})$  (Vector form)  \n","\n","> Rewriting in scalar form  \n","$z_{kl} = \\varphi_l (b_{kl} +\\sum_{j=1}^{K_{l-1}}{W}_{jkl}z_{jl−1})$  \n","The representation can be further simplified as   \n","$z_{kl} = \\varphi_l (\\sum_{j=0}^{K_{l-1}}{W}_{jkl}z_{jl−1})$  \n","by setting $z_{0l−1}=1$ and ${W}_{0kl}=b_{kl}$  \n","\n","Putting back in vector form:  \n","$\\mathbf{\\mathit{z}}_l = \\varphi_l (\\mathbf{W}_l z_{l−1})$\n","\n","</td>\n","<td style=\"width:50%; vertical-align:top;\">\n","\n","### General architecture of FFNN\n","\n","* has an input layer representing the features with dimensionality equal to the number of input features,  \n","  i.e., $z_0 =\\mathbf{x}= [x_1, \\dots, x_n]$\n","* has an output layer representing the predicted variable with dimensionality equal to the output variable,  \n","  i.e., $z_L =\\mathbf{\\hat{y}}$; $L$ is the last layer\n","* has layers $l=1$ to $l=L-1$ as the hidden layers\n","* each hidden layer $l$ has $K_l$ number of nodes\n","* $z_{kl}$ is node $k$ in layer $l$\n","* $\\mathbf{\\mathit{z}}_l$ is a vector of nodes at layer $l$\n","* $b_{kl}$ is the bias node connecting to each node $k$ in layer $l$\n","* $\\mathbf{W}_l$ is a matrix of coefficients; ${W}_{jkl}$ is coefficient from node $j$ in layer $l-1$ to node $k$ in layer $l$\n","* hidden units $z_l$ at each layer $l$ are a linear transformation of the hidden units at the previous layer  \n","  passed elementwise through an activation function\n","* $\\varphi$ is an activation function (could be any differentiable function to add non-linearity; sigmoid, tanh, ReLU are common)\n","* typically a **fully connected** feedforward network (arrow from every node in layer $l-1$ to every node in layer $l$)\n","\n","</td>\n","</tr>\n","</table>"],"metadata":{"id":"MpEPyew6-JrL"}},{"cell_type":"markdown","source":["### Example\n"," $\\mathbf{y}=f(\\mathbf{x});\\mathbf{x}\\in \\mathbb{R}^5; \\mathbf{y}\\in\\mathbb{R}^1$  where $\\sigma=  φ_l (b_{kl} +\\sum_{j=1}^{K_{l-1}}{W}_{jkl}z_{jl−1}) $ denoting the activiation function\n"," ![](https://raw.githubusercontent.com/chaitragopalappa/MIE590-690D/main/images/supply_chain_nn.png)\n"],"metadata":{"id":"xbhg4x4T9qwu"}},{"cell_type":"markdown","source":["## **Optimizer**\n","We can apply [stochastic gradient descent (SGD)](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=90ILVxtj9MRl)   \n","$\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_t − η_t∇_\\mathbf{\\theta}\\mathcal{L}(\\mathbf{\\theta}_t)$  \n","As discussed above\n","* $\\mathbf{\\theta}=[\\mathbf{b},\\mathbf{W_1},\\mathbf{W_2},...,\\mathbf{W_L}]$ are the coefficients of the neural network, so we will have to apply SDG to each coefficient\n","\n","\n","SGD application:\n","* If we can estimate $∇_\\mathbf{\\theta}\\mathcal{L}(\\mathbf{\\theta}_t)$ we can apply SDG\n","* How to estimate gradients of such complex function?\n","  * $\\hat{y}= f_L \\circ f_{L-1}\\circ f_{L-2}.....\\circ f_1$\n","  * $\\mathcal{L}= ||{\\hat{y}-y}||$ also written as $\\mathcal{L((\\mathbf{x},y),\\theta)}= ||{\\hat{y}-y}||$\n","  * $\\mathcal{L} \\equiv f_L \\circ f_{L-1}\\circ f_{L-2}.....\\circ f_1$\n","* Symbolic differentiation is tedious given the complex function (composition of functions)\n","\n","### Autodiff\n"," * Autodiff or automatic differentiation breaks a function into sequence of simple operators (that can be represented in a computational graph structure) and applies chain rule from calculus to sequentially calcuate the gradient of each operator\n","  * The architecture of the neural network nuturally has a computational graph structure.\n","  *  The sequence of derivative calculations can be forward mode or reverse mode.\n","  * Given the nature of the NN architecture reverse model autodiff is more applicable here\n","\n","  ### Additional reference:\n","  * AutoDiff: Baydan et. al., Journal of Machine Learning Research 18 (2018) 1-43\n","  * [Slides](https://dlsyscourse.org/slides/4-automatic-differentiation.pdf)\n","\n","*The above full approach of using reverse mode autodiff with SGD is called BackProp (backpropogation) algorithm*\n","\n","\n","\n"],"metadata":{"id":"82oEw9zKeP5V"}},{"cell_type":"markdown","source":["### **Autodiff for FFNN**\n","\n","Example: For a FFNN with a single hidden-layer the objective function is\n"," $\\mathcal{L((\\textbf{x},y),\\theta)}= ||{\\hat{y}-y}||= \\frac{1}{2}||y-\\mathbf{W}_2  φ_2 (\\mathbf{W}_l\\mathbf{x})||_2^2 $\n","\n","\n"," ### Feed forward pass of the NN calculates the value of $\\mathcal{L}$\n"," The objective function can be rewritten into sequence of 4 operators (4 layers from perspective of autodiff computational graph - do not confuse with NN architecture layers)\n","\n","*Figure embeded directly from [GitHub PML by Murphy](https://github.com/probml/pml-book/blob/main/book1-figures/Figure_13.10.png)*\n"," ![](https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_13.10.png)\n","\n","\n","\n","### Feed forward pass\n","\n"," $\\mathcal{L}=f_4 \\circ f_3 \\circ f_2 \\circ f_1$    \n"," $\\mathbf{x}_2= f_1(\\mathbf{x},\\theta_1 )=\\mathbf{W}_1\\mathbf{x}$    \n"," $\\mathbf{x}_3= f_2(\\mathbf{x_2},{φ})={φ}(\\mathbf{x_2})$  \n"," $\\mathbf{x}_4= f_3(\\mathbf{x_3},\\theta_3 )=\\mathbf{W}_2\\mathbf{x}_3$  \n"," $\\mathcal{L} = f_4 (\\mathbf{x_4},y)=\\frac{1}{2}||y-x_4||_2^2 $\n","\n","### Reverse mode differentiation\n","> $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_3} = \\frac{\\partial \\mathcal{L}}{\\partial x_4} \\frac{\\partial x_4}{\\partial \\theta_3}$  \n","$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta_2}  = \\frac{\\partial \\mathcal{L}}{\\partial x_4} \\frac{\\partial x_4}{\\partial x_3} \\frac{\\partial x_3}{\\partial \\theta_2}$  \n","$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1}\n","= \\frac{\\partial \\mathcal{L}}{\\partial x_4} \\frac{\\partial x_4}{\\partial x_3} \\frac{\\partial x_3}{\\partial x_2} \\frac{\\partial x_2}{\\partial \\theta_1}\n","$\n","\n"],"metadata":{"id":"C6qKuXIup1B8"}},{"cell_type":"markdown","source":["### **Backpropagation -general algorithm**\n","$$\n","\\begin{array}{l}\n","\\textbf{Algorithm: Backpropagation for an MLP with $K$ layers used inside SDG loop} \\\\[2mm]\n","\\text{// Forward pass} \\\\\n","1: \\quad x_1 := x \\\\\n","2: \\quad \\text{for } k = 1 : K \\ \\text{do} \\\\\n","3: \\quad\\quad x_{k+1} = f_k(x_k, \\theta_k) \\\\[2mm]\n","\\text{// Backward pass} \\\\\n","4: \\quad u_{K+1} := 1 \\\\\n","5: \\quad \\text{for } k = K : 1 \\ \\text{do} \\\\\n","6: \\quad\\quad g_k := u_{k+1}^\\top \\frac{\\partial f_k(x_k, \\theta_k)}{\\partial \\theta_k} \\\\\n","7: \\quad\\quad u_k^\\top := u_{k+1}^\\top \\frac{\\partial f_k(x_k, \\theta_k)}{\\partial x_k} \\\\[2mm]\n","\\text{// Output} \\\\\n","8: Return \\quad \\mathcal{L} = x_{K+1}, \\quad \\nabla_x \\mathcal{L} = u_1, \\quad \\{\\nabla_{\\theta_k} \\mathcal{L} = g_k : k = 1 : K\\}\n","\\end{array}\n","$$"],"metadata":{"id":"X-TYrvsh5RJi"}},{"cell_type":"markdown","source":["### Backprop example for FNNN with one hidden layer using Sigmoid activation\n","[Slides](https://github.com/chaitragopalappa/MIE590-690D/blob/main/images/BackProp_Example.pdf)  \n","[Code](https://github.com/chaitragopalappa/MIE590-690D/blob/main/Codes/NN_Backpropagation_Vizual.ipynb)"],"metadata":{"id":"29XbQwe1McyF"}},{"cell_type":"markdown","source":["## **Activation functions**\n","Common used functions\n","\\begin{array}{|l|l|l|l|}\n","\\hline\n","\\textbf{Name} & \\textbf{Definition} & \\textbf{Range} & \\textbf{Reference} \\\\\n","\\hline\n","\\text{Sigmoid} & \\sigma(a) = \\frac{1}{1 + e^{-a}} & [0, 1] &  \\\\\n","\\hline\n","\\text{Hyperbolic tangent} & \\tanh(a) = 2\\sigma(2a) - 1 & [-1, 1] &  \\\\\n","\\hline\n","\\text{Softplus} & \\sigma_{+}(a) = \\log(1 + e^{a}) & [0, \\infty) & \\text{[GBB11]} \\\\\n","\\hline\n","\\text{Rectified linear unit} & \\mathrm{ReLU}(a) = \\max(a, 0) & [0, \\infty) & \\text{[GBB11; KSH12]} \\\\\n","\\hline\n","\\text{Leaky ReLU} & \\max(a, 0) + \\alpha \\min(a, 0) & (-\\infty, \\infty) & \\text{[MHN13]} \\\\\n","\\hline\n","\\text{Exponential linear unit} & \\max(a, 0) + \\min\\big(\\alpha(e^{a} - 1), 0\\big) & (-\\infty, \\infty) & \\text{[CUH16]} \\\\\n","\\hline\n","\\text{Swish} & a \\, \\sigma(a) & (-\\infty, \\infty) & \\text{[RZL17]} \\\\\n","\\hline\n","\\text{GELU} & a \\, \\Phi(a) & (-\\infty, \\infty) & \\text{[HG16]} \\\\\n","\\hline\n","\\end{array}\n","*Table Source:* Reproduced from Table 13.4 PML: An Introduction by Murphy\n","\n","<p align=\"center\">\n","  <img src=\"https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_13.14_A.png\" width=\"45%\" />\n","  <img src=\"https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_13.14_B.png\" width=\"45%\" />\n","</p>\n","\n","<p align=\"center\"><em>Figure 13.14 from PML: An Introduction by Murphy (Directly embedded figure from the textbook's GitHub repository)</em></p>\n","\n","\n","Addtional Reference: https://arxiv.org/abs/1811.03378"],"metadata":{"id":"Z5fpm4QRnR_2"}},{"cell_type":"markdown","source":["## Convergence\n","* Tune hyperparmeters -learning rate, different optimizers and their hyperparamters (See optimizers and convergence properties [here](https://colab.research.google.com/drive/1pPB_YTQ93pXyXctHPP-TMBN5woWJvV6J#scrollTo=yDiw8XyW3wh-) )\n","* Pick non-saturating activation functions if issues of vanishing/exploding gradient\n","* Initialize with different initial weights (or use different random seeds if fixing the seed)"],"metadata":{"id":"yyyCnTmhsPcw"}},{"cell_type":"markdown","source":["## Regularization"],"metadata":{"id":"mjXBMH7NswXT"}},{"cell_type":"markdown","source":["# MLP exercise problem for regression"],"metadata":{"id":"KJkIjgdgwTTY"}},{"cell_type":"code","source":["#Exercise problem"],"metadata":{"id":"GxCCaqgtu8LQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"dN8zlpezvGK1"}},{"cell_type":"markdown","source":["# MLP for classifying 2D data into 2 categories"],"metadata":{"id":"OlGpX_ONa37v"}},{"cell_type":"markdown","source":["[Tensor playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.02424&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n","\n","https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/book1/13/mlp_imdb_tf.ipynb"],"metadata":{"id":"tXnFYBptbKXG"}},{"cell_type":"markdown","source":["# MLP for heteroskedastic regression"],"metadata":{"id":"tTtKzxZqvIfy"}},{"cell_type":"markdown","source":["# **XAI**\n","{SHAP tool](https://poloclub.github.io/webshap/?model=image)"],"metadata":{"id":"E8BYsMcIVoHh"}}]}