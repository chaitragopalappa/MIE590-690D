{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/2a_NN_Backprop_SymbolicDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural networks are universal function approximators\n",
        "That is, they can fit any function with the right number of nodes and activation functions even with just one layer."
      ],
      "metadata": {
        "id": "9HSdz_vnZEA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Backpropagation algorithm\n",
        "#AI prompt: write a code for backpropagation algorithm for a x^2. Create a neural network with one hidden layer and multiple nodes. Create some samples to test the model. Add a graph to vizualize output from each hidden node, plot lines on same graph with x on x axis\n",
        "#My Modification: modifed the resulting code to use symbolic diff; used AI promts to add vizualizations for further analyses\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "#from tensorboardX import SummaryWriter\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "sfPV_Eg2y19p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate Synthetic Data and Fit a model to that data { vertical-output: true }\n",
        "X = np.linspace(-4, 4, 50).reshape(-1, 1)\n",
        "Y = X ** 2 + np.random.randn(50, 1) * 2\n",
        "#Y = sin(X)\n",
        "plt.scatter(X, Y, label='Data')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.title(' Fit a NN to predict y=f(x)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hdJQYE6fzhjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data processing and handling occur at this point (cleaning, vizualizing, normalizing, train and test sets, minibatching, etc.) { vertical-output: true }\n",
        "#Skipping these steps to keep the focus on understanding NN"
      ],
      "metadata": {
        "id": "pePNyP1fz56T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize NN parameters { vertical-output: true }\n",
        "# Define the neural network architecture\n",
        "input_size =np.shape(X)[1]\n",
        "output_size = np.shape(Y)[1]\n",
        "\n",
        "# Hyperparameters (try different values)\n",
        "#Note: Number of hidden layer fixed at 1\n",
        "hidden_layer_size = 10  # Number of nodes in hidden layer\n",
        "learning_rate = 0.001 #try 0.001\n",
        "epochs = 10000\n",
        "\n",
        "# Initialize weights and biases\n",
        "W1 = np.random.randn(input_size, hidden_layer_size)\n",
        "b1 = np.zeros((1, hidden_layer_size))\n",
        "W2 = np.random.randn(hidden_layer_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Define the activation function (sigmoid)\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the derivative of the sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n"
      ],
      "metadata": {
        "id": "r_ArNFvFz_GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Apply BackProp to training data { vertical-output: true }\n",
        "loss_history = []\n",
        "predicted_output_history = []\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# BackProp Training loop\n",
        "for epoch in range(epochs):\n",
        "  # Forward pass\n",
        "  hidden_layer_input = np.dot(X, W1) + b1\n",
        "  hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "  output_layer_input = np.dot(hidden_layer_output, W2) + b2\n",
        "  output_layer_output = output_layer_input # No activation for regression\n",
        "\n",
        "  # Compute loss (mean squared error)\n",
        "  loss = np.mean((Y - output_layer_output) ** 2)\n",
        "  writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "\n",
        "  # Calculate gradients (using Symobolic differentiation)\n",
        "  d_output = -(Y - output_layer_output)  #  e_p\n",
        "  dL_db2= np.sum(d_output, axis=0, keepdims=True)\n",
        "  dL_dW2= np.dot(hidden_layer_output.T, d_output)\n",
        "  dL_db1= np.sum(np.dot(d_output, W2.T) * sigmoid_derivative(hidden_layer_output), axis=0, keepdims=True)\n",
        "  dL_dW1= np.dot(X.T, np.dot(d_output, W2.T) * sigmoid_derivative(hidden_layer_output))\n",
        "\n",
        "  # Update weights and biases (Stochastic gradient descent update)\n",
        "  W2 -= learning_rate * dL_dW2  #\n",
        "  b2 -= learning_rate * dL_db2\n",
        "  W1 -= learning_rate * dL_dW1\n",
        "  b1 -= learning_rate * dL_db1\n",
        "\n",
        "  # Store predicted output for animation\n",
        "  if epoch % 100 == 0:\n",
        "    predicted_output_history.append(output_layer_output)\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "YwCq8r5P0V92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OWaY4c370kyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot predicted and actual data - also vizualize the outputs from each hidden node to understand the inner workings\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 2))\n",
        "# Plot actual vs predicted in the first subplot\n",
        "ax1.scatter(X, Y, label=\"Actual\", s=5)\n",
        "ax1.plot(X, output_layer_output, label=\"Predicted\")\n",
        "ax1.set_xlabel(\"X\")\n",
        "ax1.set_ylabel(\"Y\")\n",
        "ax1.set_title(\"Neural Network Output\")\n",
        "ax1.legend()\n",
        "\n",
        "# Plot the output from each hidden node in the second subplot\n",
        "for i in range(hidden_layer_size):\n",
        "  ax2.plot(X, hidden_layer_output[:,i], label=f\"Hidden Node {i+1}\")\n",
        "\n",
        "ax2.set_xlabel(\"X\")\n",
        "ax2.set_ylabel(\"Y\")\n",
        "ax2.set_title(\"Hidden Node Outputs\")\n",
        "#ax2.legend()"
      ],
      "metadata": {
        "id": "UIRwqPIq0uKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#fig, ax = plt.subplots(figsize=(4, 4))\n",
        "line_predicted, = ax3.plot(X, predicted_output_history[0], label=\"Predicted\")\n",
        "line_actual, = ax3.plot(X, Y, label=\"Actual\")\n",
        "ax3.set_xlabel(\"X\")\n",
        "ax3.set_ylabel(\"Y\")\n",
        "ax3.set_title(\"Animation\")\n",
        "ax3.legend()\n",
        "\n",
        "def animate(i):\n",
        "  line_predicted.set_data(X, predicted_output_history[i])\n",
        "  return line_predicted,\n",
        "\n",
        "ani = FuncAnimation(fig, animate, frames=len(predicted_output_history), interval=1, blit=True)\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()\n",
        "# Display the animation\n",
        "HTML(ani.to_jshtml())\n",
        "'''"
      ],
      "metadata": {
        "id": "FCRsszfJtkE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "GuHaGSGJVfmh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}