{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM67TRKBJPMSe98OQ8Yl6UB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/2suppl_MathFoundation_AutoDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Differentiation methods**\n",
        "1. Symbolic differentiation\n",
        "2. Numerical differentiation\n",
        "3. Automatic differentiation (auto diff)\n",
        "  * Auto Diff context of deep learning\n",
        "---"
      ],
      "metadata": {
        "id": "VKOJG1ss9jIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Symbolic differentiation**\n",
        "\n",
        "`Given`\n",
        "$y(x,a,b)=\\frac{1}{1+e^{-(ax+b)}} \\,,$\n",
        "\n",
        "\n",
        "Calculate derivate at the point $x = 2; a = 3; b = 1$\n",
        "\n",
        "**Steps**\n",
        "\n",
        "Apply calculus to get partial derivatives\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x}\n",
        "= \\frac{a\\,e^{-(ax+b)}}{\\left(1+e^{-(ax+b)}\\right)^2},\n",
        "\\qquad\n",
        "\\frac{\\partial y}{\\partial a}\n",
        "= \\frac{x\\,e^{-(ax+b)}}{\\left(1+e^{-(ax+b)}\\right)^2},\n",
        "\\qquad\n",
        "\\frac{\\partial y}{\\partial b}\n",
        "= \\frac{e^{-(ax+b)}}{\\left(1+e^{-(ax+b)}\\right)^2}.\n",
        "$$\n",
        "Substitute values of $x = 2; a = 3; b = 1$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KdQeQmUqATND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Numerical Differentiation**\n",
        "\n",
        "**Forward pass**\n",
        "$y = \\frac{1}{1+e^{-(ax+b)}} = \\frac{1}{1+e^{-7}} \\approx 0.99909$\n",
        "\n",
        "**Numerical derivative formula**\n",
        "\n",
        "Use finite differences:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x} \\approx \\frac{y(x+\\epsilon) - y(x-\\epsilon)}{2\\epsilon}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial a} \\approx \\frac{y(a+\\epsilon) - y(a-\\epsilon)}{2\\epsilon}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial b} \\approx \\frac{y(b+\\epsilon) - y(b-\\epsilon)}{2\\epsilon}\n",
        "$$\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4kGjEOHGHasP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Numerical differentiation example { vertical-output: true }\n",
        "import numpy as np\n",
        "# Given values\n",
        "x = 2.0\n",
        "a = 3.0\n",
        "b = 1.0\n",
        "epsilon = 1e-5\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Function y\n",
        "def y_func(x_val, a_val, b_val):\n",
        "    return sigmoid(a_val*x_val + b_val)\n",
        "\n",
        "# Numerical derivatives\n",
        "dy_dx = (y_func(x+epsilon, a, b) - y_func(x-epsilon, a, b)) / (2*epsilon)\n",
        "dy_da = (y_func(x, a+epsilon, b) - y_func(x, a-epsilon, b)) / (2*epsilon)\n",
        "dy_db = (y_func(x, a, b+epsilon) - y_func(x, a, b-epsilon)) / (2*epsilon)\n",
        "\n",
        "###########################################################################\n",
        "#@title My Hidden Code Cell\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(r\"\"\"\n",
        "`Given`\n",
        "$y(x,a,b)=\\frac{1}{1+e^{-(ax+b)}} \\,,$\n",
        "\n",
        "Calculate derivate at the point $x = 2; a = 3; b = 1$\n",
        "Take $\\epsilon = 10^{-5}$.\n",
        "\n",
        "**Forward pass**\n",
        "$y = \\frac{1}{1+e^{-(ax+b)}} = \\frac{1}{1+e^{-7}} \\approx 0.99909$\n",
        "\n",
        "**Numerical differentiation**\n",
        "$$\\frac{\\partial y}{\\partial x} \\approx \\frac{y(x+\\epsilon) - y(x-\\epsilon)}{2\\epsilon}$$\n",
        "$$\\frac{\\partial y}{\\partial a} \\approx \\frac{y(a+\\epsilon) - y(a-\\epsilon)}{2\\epsilon}$$\n",
        "$$\\frac{\\partial y}{\\partial b} \\approx \\frac{y(b+\\epsilon) - y(b-\\epsilon)}{2\\epsilon}$$\n",
        "\"\"\"))\n",
        "print (\"dy_dx=\", dy_dx, \"\\n dy_da=\", dy_da, \"\\n dy_db=\", dy_db)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "IXBo8FeGHzYB",
        "outputId": "64c2e0c8-963d-45ad-8478-fc04b5fa6a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n`Given`\n$y(x,a,b)=\\frac{1}{1+e^{-(ax+b)}} \\,,$\n\nCalculate derivate at the point $x = 2; a = 3; b = 1$  \nTake $\\epsilon = 10^{-5}$.\n\n**Forward pass**\n$y = \\frac{1}{1+e^{-(ax+b)}} = \\frac{1}{1+e^{-7}} \\approx 0.99909$\n\n**Numerical differentiation**\n$$\\frac{\\partial y}{\\partial x} \\approx \\frac{y(x+\\epsilon) - y(x-\\epsilon)}{2\\epsilon}$$\n$$\\frac{\\partial y}{\\partial a} \\approx \\frac{y(a+\\epsilon) - y(a-\\epsilon)}{2\\epsilon}$$\n$$\\frac{\\partial y}{\\partial b} \\approx \\frac{y(b+\\epsilon) - y(b-\\epsilon)}{2\\epsilon}$$\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy_dx= 0.00273066354528062 \n",
            " dy_da= 0.0018204423579692983 \n",
            " dy_db= 0.0009102211706579765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autodiff**\n",
        " * Autodiff or automatic differentiation breaks a function into sequence of simple operators (that can be represented in a computational graph structure) and applies chain rule from calculus to sequentially calcuate the gradient of each operator\n",
        "\n",
        "`Given`\n",
        "$y(x,a,b)=\\frac{1}{1+e^{-(ax+b)}} \\,,$\n",
        "\n",
        "\n",
        "Calculate derivate at the point $x = 2; a = 3; b = 1$\n",
        "\n",
        "**Forward pass**\n",
        "\n",
        "\n",
        ">$z = ax + b = 3(2) + 1 = 7$  \n",
        "$y = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-7}} \\approx 0.99909$\n",
        "\n",
        "**Backward pass**\n",
        "> $\\frac{dy}{dz} = y(1-y) \\approx 0.99909 \\times (1-0.99909) \\approx 0.00091$  \n",
        "$\n",
        "\\frac{dz}{dx} = a = 3, \\quad\n",
        "\\frac{dz}{da} = x = 2, \\quad\n",
        "\\frac{dz}{db} = 1\n",
        "$\n",
        "\n",
        "> **Combine with chain rule**\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = \\frac{dy}{dz}\\frac{dz}{dx}\n",
        "= 0.00091 \\times 3 \\approx 0.00273\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{dy}{da} = \\frac{dy}{dz}\\frac{dz}{da}\n",
        "= 0.00091 \\times 2 \\approx 0.00182\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dy}{db} = \\frac{dy}{dz}\\frac{dz}{db}\n",
        "= 0.00091 \\times 1 \\approx 0.00091\n",
        "$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "79GTvfOKDW5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Automatic differentiation example { vertical-output: true }\n",
        "import tensorflow as tf\n",
        "# Define variables\n",
        "x = tf.Variable(2.0)\n",
        "a = tf.Variable(3.0)\n",
        "b = tf.Variable(1.0)\n",
        "\n",
        "# tf.GradientTape, records operations in a context to compute gradients later. We use tf.GradientTape by placing the forward pass computations within a with block, then using tape.gradient() to extract the computed gradients\n",
        "# Autodiff with GradientTape\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    z = a * x + b\n",
        "    y = tf.sigmoid(z)\n",
        "\n",
        "# Compute gradients\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_da = tape.gradient(y, a)\n",
        "dy_db = tape.gradient(y, b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "#@title My Hidden Code Cell\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(r\"\"\"\n",
        "`Given`\n",
        "$y(x,a,b)=\\frac{1}{1+e^{-(ax+b)}} \\,,$\n",
        "\n",
        "Calculate derivate at the point $x = 2; a = 3; b = 1$\n",
        "\n",
        "**Forward pass**\n",
        ">$z = ax + b = 3(2) + 1 = 7$\n",
        "$y = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-7}} \\approx 0.99909$\n",
        "\n",
        "**Backward pass**\n",
        ">$$\\frac{dy}{dz} = y(1-y) \\approx 0.99909 \\times (1-0.99909) \\approx 0.00091$$\n",
        "$$\\frac{dy}{dx} = \\frac{dy}{dz}\\frac{dz}{dx}\n",
        "= 0.00091 \\times 3 \\approx 0.00273$$\n",
        "$$\\frac{dy}{da} = \\frac{dy}{dz}\\frac{dz}{da}\n",
        "= 0.00091 \\times 2 \\approx 0.00182$$\n",
        "$$\\frac{dy}{db} = \\frac{dy}{dz}\\frac{dz}{db}\n",
        "= 0.00091 \\times 1 \\approx 0.00091$$\n",
        "\"\"\"))\n",
        "\n",
        "\n",
        "dy_dx.numpy(), dy_da.numpy(), dy_db.numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "K_jRL_U8_xMM",
        "outputId": "50b0ef94-2d2d-467b-a814-64c3e329ea58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n`Given`\n$y(x,a,b)=\\frac{1}{1+e^{-(ax+b)}} \\,,$\n\nCalculate derivate at the point $x = 2; a = 3; b = 1$\n\n**Forward pass**\n>$z = ax + b = 3(2) + 1 = 7$  \n$y = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-7}} \\approx 0.99909$  \n\n**Backward pass**\n>$$\\frac{dy}{dz} = y(1-y) \\approx 0.99909 \\times (1-0.99909) \\approx 0.00091$$\n$$\\frac{dy}{dx} = \\frac{dy}{dz}\\frac{dz}{dx}\n= 0.00091 \\times 3 \\approx 0.00273$$\n$$\\frac{dy}{da} = \\frac{dy}{dz}\\frac{dz}{da}\n= 0.00091 \\times 2 \\approx 0.00182$$\n$$\\frac{dy}{db} = \\frac{dy}{dz}\\frac{dz}{db}\n= 0.00091 \\times 1 \\approx 0.00091$$\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float32(0.0027306809), np.float32(0.0018204539), np.float32(0.00091022695))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ### Additional reference:\n",
        "  * AutoDiff: Baydan et. al., Journal of Machine Learning Research 18 (2018) 1-43\n",
        "  * [Slides](https://dlsyscourse.org/slides/4-automatic-differentiation.pdf)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GsLGDtCbFTHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autodiff for Feed Forward Neural Network (FFNN)**\n",
        " * Autodiff or automatic differentiation breaks a function into sequence of simple operators (that can be represented in a computational graph structure) and applies chain rule from calculus to sequentially calcuate the gradient of each operator\n",
        "  * The architecture of the neural network nuturally has a computational graph structure.\n",
        "  *  The sequence of derivative calculations can be forward mode or reverse mode.\n",
        "  * Given the nature of the NN architecture reverse model autodiff is more applicable here\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bh0a1ryW0T1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Autodiff for Feed Forward Neural Network (FFNN)**\n",
        "\n",
        "Example: For a FFNN with a single hidden-layer the objective function is\n",
        " $\\mathcal{L((\\textbf{x},y),\\theta)}= ||{\\hat{y}-y}||= \\frac{1}{2}||y-\\mathbf{W}_2  φ_2 (\\mathbf{W}_1\\mathbf{x})||_2^2 $\n",
        "\n",
        "---\n",
        "\n",
        " ### Feed forward pass of the NN calculates the value of $\\mathcal{L}$\n",
        " The objective function can be rewritten into sequence of 4 operators (4 layers from perspective of autodiff computational graph - do not confuse with NN architecture layers)\n",
        "\n",
        "*Figure embeded directly from [GitHub PML by Murphy](https://github.com/probml/pml-book/blob/main/book1-figures/Figure_13.10.png)*\n",
        " ![](https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_13.10.png)\n",
        "\n",
        "\n",
        "---\n",
        "### Feed forward pass\n",
        "\n",
        "> $\\mathcal{L}=f_4 \\circ f_3 \\circ f_2 \\circ f_1$    \n",
        " $(\\mathbf{x}=\\mathbf{x}_1)$  \n",
        " $\\mathbf{x}_2= f_1(\\mathbf{x},\\theta_1 )=\\mathbf{W}_1\\mathbf{x} $    \n",
        " $\\mathbf{x}_3= f_2(\\mathbf{x_2},{φ})={φ}(\\mathbf{x_2})$  \n",
        " $\\mathbf{x}_4= f_3(\\mathbf{x_3},\\theta_3 )=\\mathbf{W}_2\\mathbf{x}_3$  \n",
        " $o=\\mathcal{L} = f_4 (\\mathbf{x_4},y)=\\frac{1}{2}||y-x_4||_2^2 $\n",
        "\n",
        "### Reverse mode differentiation\n",
        "> $\\frac{\\partial \\mathcal{L}}{\\partial x_4} = \\frac{\\partial \\mathcal{L}}{\\partial x_4}$  \n",
        "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_3} = \\frac{\\partial \\mathcal{L}}{\\partial x_4} \\frac{\\partial x_4}{\\partial \\theta_3}$  \n",
        "$\\frac{\\partial \\mathcal{L}}{\\partial x_3}  = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_3} \\frac{\\partial \\theta_3}{\\partial x_3}$  \n",
        "$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta_2}  = \\frac{\\partial \\mathcal{L}}{\\partial x_3} \\frac{\\partial x_3}{\\partial \\theta_2}$   \n",
        "$\\frac{\\partial \\mathcal{L}}{\\partial x_2}  = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_2} \\frac{\\partial \\theta_2}{\\partial x_2}$     \n",
        "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1}\n",
        "= \\frac{\\partial \\mathcal{L}}{\\partial x_2}  \\frac{\\partial x_2}{\\partial \\theta_1}\n",
        "$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CT7XqMSj0bBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Backpropogation for above example**\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\textbf{Algorithm: Backpropagation for an MLP with $K$ layers used inside SDG loop} \\\\[2mm]\n",
        "\\text{// Forward pass} \\\\\n",
        "\\quad \\mathbf{x}_1 := \\mathbf{x} \\\\\n",
        "\\quad \\mathbf{x}_2= f_1(\\mathbf{x},\\theta_1 )=\\mathbf{W}_1\\mathbf{x}    \\\\\n",
        "\\quad \\mathbf{x}_3= f_2(\\mathbf{x_2},{φ})={φ}(\\mathbf{x_2})\\\\\n",
        "\\quad \\mathbf{x}_4= f_3(\\mathbf{x_3},\\theta_3 )=\\mathbf{W}_2\\mathbf{x}_3 \\\\\n",
        "\\quad o=\\mathcal{L} = f_4 (\\mathbf{x_4},y)=\\frac{1}{2}||y-x_4||_2^2 \\\\\n",
        "\\\\[2mm]\n",
        "\\text{// Backward pass} \\\\\n",
        "\\quad u_{4+1} := 1 ;\\\\\n",
        "\\quad \\frac{\\partial \\mathcal{L}}{\\partial \\theta_4} = g_4 := u_{4+1}^\\top \\frac{\\partial f_4(x_4, \\theta_4)}{\\partial \\theta_4} = u_{4+1}^\\top \\frac{\\partial \\mathcal{L}}{\\partial \\theta_4} =0\\\\\n",
        "\\quad  \\frac{\\partial \\mathcal{L}}{\\partial x_4} = u_4^\\top := u_{4+1}^\\top \\frac{\\partial f_4(x_4, \\theta_4)}{\\partial x_4}= u_{4+1}^\\top \\frac{\\partial \\mathcal{L}}{\\partial x_4}  =-(y-x_4)\\\\\n",
        "\\quad \\frac{\\partial \\mathcal{L}}{\\partial \\theta_3} = g_3 := u_{3+1}^\\top \\frac{\\partial f_3(x_3, \\theta_3)}{\\partial \\theta_3} = u_{3+1}^\\top \\frac{\\partial \\mathcal{x_4}}{\\partial \\theta_3} =\\frac{\\partial \\mathcal{L}}{\\partial x_4}\\frac{\\partial \\mathcal{x_4}}{\\partial \\theta_3}  =\\frac{\\partial \\mathcal{L}}{\\partial x_4} x_3 ( \\equiv \\frac{\\partial \\mathcal{L}}{\\partial W_2})\\\\\n",
        "\\quad \\frac{\\partial \\mathcal{L}}{\\partial x_3} = u_3^\\top := u_{3+1}^\\top \\frac{\\partial f_3(x_3, \\theta_3)}{\\partial x_3}= u_{3+1}^\\top \\frac{\\partial \\mathcal{x_4}}{\\partial x_3} = \\frac{\\partial \\mathcal{L}}{\\partial x_4}\\frac{\\partial \\mathcal{x_4}}{\\partial x_3}= \\frac{\\partial \\mathcal{L}}{\\partial x_4}W_2\\\\\n",
        "\\quad \\frac{\\partial \\mathcal{L}}{\\partial \\theta_2} = g_2 := u_{2+1}^\\top \\frac{\\partial f_2(x_2, \\theta_2)}{\\partial \\theta_2} = u_{2+1}^\\top \\frac{\\partial \\mathcal{x_3}}{\\partial \\theta_2}=\\frac{\\partial \\mathcal{L}}{\\partial x_4}\\frac{\\partial \\mathcal{x_4}}{\\partial x_3}\\frac{\\partial \\mathcal{x_3}}{\\partial \\theta_2} = \\frac{\\partial \\mathcal{L}}{\\partial x_3}\\frac{\\partial{φ}}{\\partial{φ}}\\\\\n",
        "\\quad \\frac{\\partial \\mathcal{L}}{\\partial x_2} = u_2^\\top := u_{2+1}^\\top \\frac{\\partial f_2(x_2, \\theta_2)}{\\partial x_2}= u_{2+1}^\\top \\frac{\\partial \\mathcal{x_3}}{\\partial x_2} =\\frac{\\partial \\mathcal{L}}{\\partial x_4}\\frac{\\partial \\mathcal{x_4}}{\\partial x_3} \\frac{\\partial \\mathcal{x_3}}{\\partial x_2}=\\frac{\\partial \\mathcal{L}}{\\partial x_3}\\frac{\\partial{φ}}{\\partial x_2}\\\\\n",
        "\\quad \\frac{\\partial \\mathcal{L}}{\\partial \\theta_1} = g_1 := u_{1+1}^\\top \\frac{\\partial f_1(x_1, \\theta_1)}{\\partial \\theta_1} = u_{1+1}^\\top \\frac{\\partial \\mathcal{x_2}}{\\partial \\theta_1}=\\frac{\\partial \\mathcal{L}}{\\partial x_4}\\frac{\\partial \\mathcal{x_4}}{\\partial x_3} \\frac{\\partial \\mathcal{x_3}}{\\partial x_2}\\frac{\\partial \\mathcal{x_2}}{\\partial \\theta_1} = \\frac{\\partial \\mathcal{L}}{\\partial x_2}x_1 ( \\equiv \\frac{\\partial \\mathcal{L}}{\\partial W_1})\\\\\n",
        "\\quad \\frac{\\partial \\mathcal{L}}{\\partial x_1} = u_1^\\top := u_{1+1}^\\top \\frac{\\partial f_1(x_1, \\theta_1)}{\\partial x_1}= u_{1+1}^\\top \\frac{\\partial \\mathcal{x_2}}{\\partial x_1}=\\frac{\\partial \\mathcal{L}}{\\partial x_4}\\frac{\\partial \\mathcal{x_4}}{\\partial x_3} \\frac{\\partial \\mathcal{x_3}}{\\partial x_2} \\frac{\\partial \\mathcal{x_2}}{\\partial x_1} \\\\\n",
        "\\\\[2mm]\n",
        "\\text{// Output} \\\\\n",
        "Return \\quad \\mathcal{L} = x_{K+1}, \\quad \\nabla_x \\mathcal{L} = u_1= \\frac{\\partial \\mathcal{L}}{\\partial x_1}, \\quad \\{\\nabla_{\\theta_k} \\mathcal{L} = g_k = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_k}: k = 1 : K\\} \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "79bNqugq8tj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Backpropagation -general algorithm**\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "\\textbf{Algorithm: Backpropagation for an MLP with $K$ layers used inside SDG loop} \\\\[2mm]\n",
        "\\text{// Forward pass} \\\\\n",
        "1: \\quad x_1 := x \\\\\n",
        "2: \\quad \\text{for } k = 1 : K \\ \\text{do} \\\\\n",
        "3: \\quad\\quad x_{k+1} = f_k(x_k, \\theta_k) \\\\[2mm]\n",
        "\\text{// Backward pass} \\\\\n",
        "4: \\quad u_{K+1} := 1 \\\\\n",
        "5: \\quad \\text{for } k = K : 1 \\ \\text{do} \\\\\n",
        "6: \\quad\\quad g_k := u_{k+1}^\\top \\frac{\\partial f_k(x_k, \\theta_k)}{\\partial \\theta_k} \\\\\n",
        "7: \\quad\\quad u_k^\\top := u_{k+1}^\\top \\frac{\\partial f_k(x_k, \\theta_k)}{\\partial x_k} \\\\[2mm]\n",
        "\\text{// Output} \\\\\n",
        "8: Return \\quad \\mathcal{L} = x_{K+1}, \\quad \\nabla_x \\mathcal{L} = u_1, \\quad \\{\\nabla_{\\theta_k} \\mathcal{L} = g_k : k = 1 : K\\}\n",
        "\\end{array}\n",
        "$$"
      ],
      "metadata": {
        "id": "lYC4xjzc8thy"
      }
    }
  ]
}