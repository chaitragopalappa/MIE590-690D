{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxQG51C4GQZfl4rluxOXO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/3_Lab_NN_tabular_data_DeeperDive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks for Structured Data or Tabular data\n",
        "Feed Forward Neural Network (FFNN) / Multi-layer Perceptrons (MLP)\n",
        "References:\n",
        "* Chapter 13, Probabilistic Machine Learning: An Introduction by Kevin Murphy  \n",
        "\n"
      ],
      "metadata": {
        "id": "-e1YX7UMzVku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Review slide sets\n",
        "* 2_c_code (autodiff)\n",
        "* 2_d_code (pytorch basics) - this has detailed comments at every line - so use this to understand the use of NN packages (though syntax change with different packages the overall approach is similar in that there is NN pipeline defined in a class with NN.module as superclass, you create an object of the class to access NN weights (parameters), and use inbuilt function for calculating loss, calculating gradients, and using optimizer.\n",
        "* 3_Lecture\n",
        "* 3suppl (Optimizers) - slides for this are on Canvas"
      ],
      "metadata": {
        "id": "bI9IDiOyMViP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Exercise - Experiment with the various components of NN\n",
        "\n",
        "Use a toy data set (e.g., y=x^2 or y= sin(x)) to experiment below\n",
        "* Activation functions\n",
        "* Optimizers\n",
        "* Hyperparameters (inputs to different optimizers, NN architecture)\n",
        "* Regularization (early stop, weight decay, and drop-out)\n",
        "You can use/expand on the code 2d_Code_Pytorch_basics.ipynb or write your own code.\n",
        "Use vizuals where needed - e.g., plot input and outsignals from every node in hidden layers to unerstand role of activation fucntions.\n"
      ],
      "metadata": {
        "id": "OlGpX_ONa37v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Exercise - MLP for classification problem\n",
        "Below is a code for a classification problem. Your assignment is to generate two versions of a MLP, a wrong model, and a correct model\n",
        "- A wrong_model that uses 1 node in the ouput, i.e., the output is the 'class'\n",
        "- A correct_model where the output layer has size equal to number of classes- the output is a probability distribution.\n",
        "\n",
        "Observe the outputs from each code by plotting relevant output metrics of both versions onto the same plot. Why does the 'correct_model' have a better performance?"
      ],
      "metadata": {
        "id": "V-RBYA2WNCj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Ungraded quiz\n",
        "I will post a quiz on Monday and open it until end of Monday. This would be part of your participation grade, but you will not be deducted marks for wrong answer. It will ask questions related to the assignments in this Lab"
      ],
      "metadata": {
        "id": "ykGQHvOgkf3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MIscellaneous\n",
        "1. MLP playground - use this to experiment and observe the inner workings of NN\n",
        "[Tensor playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.02424&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
        "\n",
        "2.  MLP for text data - [MLP for text data (binary sentiment analyses)](https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/book1/13/mlp_imdb_tf.ipynb)\n",
        "\n",
        "3. MLP exercise using Keras- [In-depth ](https://github.com/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb)\n",
        "Review the following\n",
        "* Avoiding vanishing/exploding graident issues of sigmoid (use non-saturating activation functions, gradient clipping, batch normalization)\n",
        "* Faster Optimizers (Momentum optimization, Nestorov momentum, AdaGrad, RMSProp, Adam optimization, Adamax optimization, Nadam optimization)\n",
        "* Regularization to avoid overfitting (L1 and L2 regularization, drop-out, Alpha dropout, MC dropout, Max Norm)\n",
        "\n",
        "4. [Data handling for tabular, image, and text data](https://github.com/ageron/handson-ml3/blob/main/13_loading_and_preprocessing_data.ipynb)"
      ],
      "metadata": {
        "id": "KJkIjgdgwTTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "dN8zlpezvGK1"
      }
    }
  ]
}