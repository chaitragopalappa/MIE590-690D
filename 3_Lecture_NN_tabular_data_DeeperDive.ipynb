{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPiiV5DHOaHHlJfQ334wiQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/3_Lecture_NN_tabular_data_DeeperDive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuts and Bolts - Neural Networks for Structured Data or Tabular data\n",
        "Feed Forward Neural Network (FFNN) / Multi-layer Perceptrons (MLP)\n",
        "References:\n",
        "* Chapter 13, Probabilistic Machine Learning: An Introduction by Kevin Murphy  \n",
        "* Bengio, Y., Practical recommendations for gradient-based training of deep architectures, 2012, https://arxiv.org/abs/1206.5533\n",
        "* Nawankpa, C., Activation Functions: Comparison of trends in Practice and Research for Deep Learning, 2017, https://doi.org/10.48550/arXiv.1811.03378\n"
      ],
      "metadata": {
        "id": "-e1YX7UMzVku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recommended readings**\n",
        "* Bengio, Y., Practical recommendations for gradient-based training of deep architectures, 2012, https://arxiv.org/abs/1206.5533\n",
        "* Nawankpa, C., Activation Functions: Comparison of trends in Practice and Research for Deep Learning, 2017, https://doi.org/10.48550/arXiv.1811.03378\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3ALCaL26d0LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Outline**\n",
        "* Data handling\n",
        "* Data preprocessing\n",
        "* Split data into test and train  \n",
        "* Batching of train set\n",
        "* Activation functions\n",
        "* Regularization (avoid overfitting)\n",
        "* Optimizers\n",
        "* Hyperparameter tuning\n",
        "* Loss functions\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FqGItbS75U0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data preprocessing**\n",
        " * Data standardization, or Z-score normalization, scales data to have a mean of 0 and a standard deviation of 1, preserving the distribution shape but without a fixed range.\n",
        " * Data normalization, such as Min-Max Scaling, scales data to a specific range, typically 0 to 1, which can alter the distribution's shape and is useful for algorithms that require data within a certain range\n",
        "\n",
        " In deep learning we prefer standardization aas it preserves the distribution\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "nEVELWkcmiGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train and test sets**\n",
        "* Typical to split data into train and test sets\n",
        "* Use ‘train’ set to train the data\n",
        "* Test the trained model on the ‘test’ set\n",
        "---"
      ],
      "metadata": {
        "id": "OftY3QWDaioe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Batching of train set**\n",
        "\n",
        "* Batch: use the full train set to train the model\n",
        "* Mini-batch: divide train set into small mini-batches\n",
        "  * Typically $2^n$: 32, 64, 128, 256 (corresponding to CPU /GPU architecture)\n",
        "* Incremental/ online learning: single sample at a time\n",
        "\n",
        "Bengio, Practical recommendations for gradient-based training of deep architectures, 2012, https://arxiv.org/abs/1206.5533\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "A4246uJ0aX3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Activation functions**\n",
        "Common used functions\n",
        "\\begin{array}{|l|l|l|l|}\n",
        "\\hline\n",
        "\\textbf{Name} & \\textbf{Definition} & \\textbf{Range} & \\textbf{Reference} \\\\\n",
        "\\hline\n",
        "\\text{Sigmoid} & \\sigma(a) = \\frac{1}{1 + e^{-a}} & [0, 1] &  \\\\\n",
        "\\hline\n",
        "\\text{Hyperbolic tangent} & \\tanh(a) = 2\\sigma(2a) - 1 & [-1, 1] &  \\\\\n",
        "\\hline\n",
        "\\text{Softplus} & \\sigma_{+}(a) = \\log(1 + e^{a}) & [0, \\infty) & \\text{[GBB11]} \\\\\n",
        "\\hline\n",
        "\\text{Rectified linear unit} & \\mathrm{ReLU}(a) = \\max(a, 0) & [0, \\infty) & \\text{[GBB11; KSH12]} \\\\\n",
        "\\hline\n",
        "\\text{Leaky ReLU} & \\max(a, 0) + \\alpha \\min(a, 0) & (-\\infty, \\infty) & \\text{[MHN13]} \\\\\n",
        "\\hline\n",
        "\\text{Exponential linear unit} & \\max(a, 0) + \\min\\big(\\alpha(e^{a} - 1), 0\\big) & (-\\infty, \\infty) & \\text{[CUH16]} \\\\\n",
        "\\hline\n",
        "\\text{Swish} & a \\, \\sigma(a) & (-\\infty, \\infty) & \\text{[RZL17]} \\\\\n",
        "\\hline\n",
        "\\text{GELU} & a \\, \\Phi(a) & (-\\infty, \\infty) & \\text{[HG16]} \\\\\n",
        "\\hline\n",
        "\\end{array}\n",
        "*Table Source:* Reproduced from Table 13.4 PML: An Introduction by Murphy\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_13.14_A.png\" width=\"45%\" />\n",
        "  <img src=\"https://raw.githubusercontent.com/probml/pml-book/main/book1-figures/Figure_13.14_B.png\" width=\"45%\" />\n",
        "</p>\n",
        "\n",
        "<p align=\"center\"><em>Figure 13.14 from PML: An Introduction by Murphy (Directly embedded figure from the textbook's GitHub repository)</em></p>\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Z5fpm4QRnR_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vanishing/Exploding Gradient Issues**\n",
        "When training very deep models, the gradient tends to become either very small (this is called the vanishing gradient problem) or very large (this is called the exploding gradient problem).\n",
        "\n",
        "Solutions:\n",
        "* INitialize weights to be not too high or too low\n",
        "* The exploding gradient problem can be fixed by **gradient clipping** $g' = min(1,\\frac{c}{||g||})g $  \n",
        "  * $g$ is the gradent at some layer; the equation ensures that the norm of $ g$ is never greater than some constant $c$; and multiplying by $g$ ensures the vector is in same direction as $g$\n",
        "* Use non-saturating activation functions\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ig9HtIt1Q5lB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saturating functions: Sigmoid function and Tanh activation functions**\n",
        "\n",
        "Sigmoid saturate at 1 for large positive inputs, and at 0 for large negative inputs. Tanh function, which has a similar shape, but saturates at -1 and +1.\n",
        "In the saturated regimes, the gradient of the output wrt the input will be close to zero, so any gradient signal from higher layers will not be able to propagate back to earlier layers. This is called\n",
        "the vanishing gradient problem. Solution is to use **non-saturating** functions\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9r5RV_PvCMxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**ReLU (Rectified Linear Unit)**\n",
        "\n",
        "The ReLU function simply “turns off” negative inputs, and passes positive inputs unchanged.\n",
        "$ReLU(a) = max(a, 0) =a\\mathbb{I}(a>0); \\mathbb{I}$ is an indicator function.  \n",
        "$ReLU'(a) = \\mathbb{I}(a>0)$  \n",
        "Suppose $z = ReLU(Wx)$   \n",
        "$\\frac{\\partial z}{\\partial W}=\\mathbb{I}(a>0)x^T$\n",
        "\n",
        "**Dead ReLU problem**\n",
        "\n",
        "When using ReLU, if the weights are initialized such that a = Wx take on large negative values, then the signal outputs from neurons are never activated, and the signal dies out . This is called the **dead ReLU** problem\n",
        "\n",
        "**Leaky ReLU : Non-saturating version of ReLU**  \n",
        "Overcomes dead ReLU problem.\n",
        "$LReLU(a; \\alpha) = max(\\alpha a, a)$  \n",
        "where $0 < \\alpha < 1$. The slope of this function is 1 for positive inputs, and $\\alpha$ for negative inputs, thus ensuring there is some signal passed back to earlier layers, even when the input is negative.\n",
        "\n",
        "$\\alpha$ can be **hyperparamter**\n",
        "\n",
        "**Exponential Linear Unit (ELU)**\n",
        "$$\n",
        "\\mathrm{ELU}(a; \\alpha) =\n",
        "\\begin{cases}\n",
        "\\alpha \\big(e^{a} - 1\\big), & \\text{if } a \\leq 0 \\\\\n",
        "a, & \\text{if } a > 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Self-normalizing ELU**  \n",
        "$SELU(a; α, λ) = λELU(a; α)$  \n",
        "The authors prove that by setting $\\alpha$ and $\\lambda$ to carefully chosen values, this activation function is guaranteed to ensure that the output of each layer is standardized (provided the input is also standardized), even without the use of techniques such as batchnorm\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jq5iqIJ-CYDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Regularization- steps to avoid over-fitting**\n",
        "1. **Early stop**: stopping the training procedure when the error on the validation set starts to increase\n",
        "2. **Weight decay**:  This is equivalent to L2 regularization-  by adding a penalty to the loss function based on the sum of the squares of the model's weights. In the neural networks literature, this is called weight decay, since it encourages small weights, and hence simpler models, as in ridge regression. (This is equivalent to using a Gaussian prior for the weights $\\mathcal{N} (w|0, α^2I)$ and biases,$\\mathcal{N} (b|0, β^2I)$.)\n",
        "  *  IN NN packages this is an input to the Optimizer\n",
        "3. **Drop-out**: Turn off all the outgoing connections from each neuron with probability $p$. Can dramatically reduces over-fitting and us widely used. Intuitively, each unit must learn to perform well even if some of the other units are missing at random.\n",
        "\n",
        "___\n"
      ],
      "metadata": {
        "id": "ZfVMy6UPStKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optimizers (See SLIDES on Canvas)**\n",
        "Different options for search direction and learning rate (step size)\n",
        "### Line search methods\n",
        "Methods that pick a search direction and step in that direction with some step size. In ML step-size are typically referred to as learing rate\n",
        "* Gradient as search direction\n",
        "  * SDG\n",
        "  * SDG with momentum\n",
        "  * SDG with Nestrov\n",
        "* Adaptive learning rate (step size)\n",
        "  * AdaGrad\n",
        "  * RMSProp\n",
        "  * Adam\n",
        "* Hessian as search direction\n",
        "  * Newtons method\n",
        "  * Conjugate gradients\n",
        "  * BFGS\n",
        "\n",
        "### Trust-region methods\n",
        "Methods that determine search direction and step-size together\n",
        "* Levenberg Marquardt\n",
        "\n",
        "**See SLIDES on Canvas**\n",
        "\n",
        "### References\n",
        "[Algorithms](https://www.deeplearningbook.org/contents/optimization.html) Algorithms 8.1 to 8.7 from Chapter 8: Ian Goodfellow and Yoshua Bengio and Aaron Courville, Deep Learning  \n",
        "[Computational implementation in Pytorch](https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)  \n",
        "\n",
        "[Vizualization tool](https://github.com/lilipads/gradient_descent_viz)\n",
        "\n",
        "\n",
        "(See convergence properties for optimizers [1suppl_Mathematical Foundations of ML](https://github.com/chaitragopalappa/MIE590-690D/blob/main/suppl_files/1suppl_Mathematical_foundations_of_ML.ipynb)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lJPsQ6p78XK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizers -Additional variants**\n",
        "\n",
        "New variants are continuoualy added- Best way to keep up is to look at NN libraries (Keras, Pytorch, Tensorflow) on available options\n",
        "* [Kieras](https://keras.io/api/optimizers/)  \n",
        "* [Pytorch](https://pytorch.org/docs/stable/optim.html)  \n",
        "* [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers )  \n",
        "\n",
        "Bengio, Practical recommendations for gradient-based training of deep architectures, 2012, https://arxiv.org/abs/1206.5533\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eApVDV00S2N7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hyperparameter tuning summary -for convergence**\n",
        "* Initialize with different initial weights (or use different random seeds if fixing the seed)\n",
        "* Pick non-saturating activation functions if issues of vanishing/exploding gradient\n",
        "* Tune hyperparmeters - parameters related to optimizer, weight decay (regularization weights)\n",
        "* Use different optimizers\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "yyyCnTmhsPcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Architecture for regression v. classification**"
      ],
      "metadata": {
        "id": "cw8MOYi4gGpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Architecture: MLP on tabular data : regression problem**\n",
        "\n",
        "\n",
        " $y=f(\\mathbf{x})$  \n",
        " Predict:  \n",
        " $\\mathbf{z}_L= y= (\\mathbf{b}_{L} +\\mathbf{W}_{L}\\mathbf{z}_{L−1})$  \n",
        " $\\mathbf{z}_l=  \\phi_l (\\mathbf{b}_{l} +\\mathbf{W}_{l}\\mathbf{z}_{l−1})$ (matrix form) for $l=1:L-1$ hidden layers; $z_0=x$"
      ],
      "metadata": {
        "id": "vbUYz7tLCjgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Architecture: MLP for tabular data : classification problem**\n",
        "\n",
        "**TWo classes**  \n",
        "Predict:  \n",
        " $p(y|x; θ) = Ber(y|σ(a_L)) $  \n",
        " $\\sigma(a)$ is sigmoid activation function  \n",
        "$a_L = w^T_L z_{L-1} + b_L$  \n",
        " $\\mathbf{z}_l=  \\phi_l (\\mathbf{b}_{l} +\\mathbf{W}_{l}\\mathbf{z}_{l−1})$ (matrix form) for $l=1:L-1$ hidden layers; $z_0=x$\n",
        "\n",
        " $a_L$, i.e., the output from the final layer are called the **logit score**  \n",
        " IN statistics, a logit is the natural logarithm of the odds, where odds are the ratio of the probability of an event occurring to the probability of it not occurring. The logit function transforms a probability (a value between 0 and 1) into a value that can range from negative to positive infinity. IN deep learning, we do the inverse, pass teh logist either through sigmoid (for 2-class problem) or softmax (for N-class problem) to convert outputs to a probability dsitribution. Thus, the output from the NN are equivalent to learning the logits.\n",
        "\n",
        " **Multi-class classification (> 2 classes)**  \n",
        "Same as above except logit scores are passed through a soft-max function to generate a probaility distrinution that adds to 1 (sigmoid only converts it into values between 0 and 1, so only used in 2-class problem. In 2-class problem, if we know probability of class 1 (p) we can calculate probability of class 2 as  1-p)\n",
        "\n",
        "In NN packages: soft-max is typically integrated into the cross-entropy loss, and thus, in setting-up NN archtecture,  we need not dpass the final layer through soft-max.\n",
        "\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "OlGpX_ONa37v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss function for MLP regression v. classification**"
      ],
      "metadata": {
        "id": "kWM_UtpZgYX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loss function for regression - MSE**\n",
        "Objective function: $ Min\\mathcal{L}(\\mathbf{\\theta}) =Min_\\mathbf{\\theta}||\\mathbf{\\hat{y}-y}||_2^2$"
      ],
      "metadata": {
        "id": "cSgzA-GxgiT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loss function for classification- cross-entropy or log-loss**\n",
        "\n",
        "* Cross-Entropy loss or log-loss\n",
        "  * Binary output (2 classes): Suppose ouput $y$ takes values 1 or 0, and $p_i$ is the model predicted probability of label for observation $i$, then entropy loss is\n",
        "  $$-\\frac{1}{N}\\sum_{i=1}^N (y_i log(p_i)+(1-y_i)log(1-p_i))$$\n",
        "\n",
        "  * Categorical output (>2 classes): $y_{i,c}=1$ if true class is $c$ and $p_{i,c}$ is the model predicted probability\n",
        "   $$ -\\sum_c y_{i,c} log(p_{i,c})$$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Jj_qX5-lW-z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dN8zlpezvGK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP for heteroskedastic regression\n",
        "“Heteroskedastic”  means that the predicted output variance is input-dependent. This function has two outputs which compute $f_{\\mu}(x) = \\mathbb{E} [y|x, θ]$ and $f_{\\sigma}(x) = \\sqrt{\\mathbb{V} [y|x, θ]}$.\n",
        "Most of the layers (and hence parameters) can be shared between the two functions by using a common “backbone” and two output “heads”. A linear activation for the $\\mu$ head, a softplus activation for the $\\sigma$ head are typically used.\n",
        "![](https://brendanhasz.github.io/assets/img/dual-headed/TwoHeadedNet.svg)\n",
        "Source: Brandan Hasz, Trip Duration Prediction using Bayesian Neural Networks and TensorFlow 2.0, https://brendanhasz.github.io/2019/07/23/bayesian-density-net.html\n"
      ],
      "metadata": {
        "id": "tTtKzxZqvIfy"
      }
    }
  ]
}