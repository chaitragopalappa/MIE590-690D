{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLYOZPCM2oVreoUeHtc0M6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitragopalappa/MIE590-690D/blob/main/4_lab_XAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIME**\n",
        "  * [Code 1- Regression using RandomForests](https://marcotcr.github.io/lime/tutorials/Using%2Blime%2Bfor%2Bregression.html) ;\n",
        "\n",
        " * [Code 2- Classification using RandomForests and XGBoost](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html) ;\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6Ys-GNsMBcQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHAP**\n",
        "\n",
        "**Learn how to interpret the SHAP plots**:\n",
        "* Example 1: Review the Penguin example at the end of SHAP chapter: [Interpretable Machine Learning: A Guide for Making Black Box Models Explainable, by Christoph Molnar](https://christophm.github.io/interpretable-ml-book/shap.html#kernelshap)\n",
        "* Example 2: [SHAP GitHUB example](https://github.com/shap/shap?tab=readme-ov-file#tree-ensemble-example-xgboostlightgbmcatboostscikit-learnpyspark-models)\n",
        "* Example 3: [Explainable AI, by Aidan Cooper](https://www.aidancooper.co.uk/a-non-technical-guide-to-interpreting-shap-analyses/?xgtab&)\n",
        "\n",
        "**What to review in above examples**:  \n",
        "* SHAP force plot and waterfall plot for feature attributions of individual instance/sample\n",
        "  * Shapley values are calculated for each instance w.r.t. a baseline (average or random selection) using the method discussed in the lecture (KernelSHAP, TreeSHAP, or permutation method).\n",
        "\n",
        "* **SHAP aggregation plots**: Shapley values can be combined into **global explanations**. If we run SHAP for every instance, we get a matrix of Shapley values (one row per data instance and one column per feature). We can interpret the entire model by analyzing the Shapley values in this matrix.\n",
        "  * SHAP feature importance\n",
        "  * SHAP summary plot\n",
        "  * SHAP dependence plot\n",
        "  * SHAP interaction value (dependence plot )\n",
        "  * SHAP interaction value summary plot\n",
        "\n",
        "\n",
        "* Based on understanding of above plots, **write a short para of the key findings from the SHAP plots** of the [NHANES survival hazard prediction model](\n",
        "https://shap.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html) **YOU CANNOT USE AI TO PROMPT THIS RESPONSE**\n",
        "  * This interpret a XGBoost models which is a tree-based ML method. it uses TreeSHAP. If we had individual level data for food insecurity (in your HW 1 we only have aggregated county-level data), we could apply XGBoost and interpret using TreeSHAP.\n",
        "\n",
        "* **Apply SHAP to your HW 1**:\n",
        "  *  As discussed in lecture, the general model-agnostic algorithms to compute SHAP include kernelSHAP, TreeSHAP, and permutation method. The above examples use the inbuilt class [shap.kernelexplainer](https://github.com/shap/shap?tab=readme-ov-file#model-agnostic-example-with-kernelexplainer-explains-any-function))\n",
        "  * shap.deepexplainer or shap.gradientexplainer, are based on algorithms specifc to deeplearning models - they are more computationally efficient as they utilize the computational design of DL (we will see these concepts in the  next lecture).\n",
        "  * You can use shap.kernelexplainer, shap.deepexplainer, or shap.gradientexplainer ; you can use AI to help write the code (Note: As always, though you can use AI to help you with syntax specific to these packages. But you do need to understand what the code is doing. As in Exam 1, you will be tested on basic knowledge)\n",
        "  *  **Write a short para of the key findings from the SHAP plots**  **YOU CANNOT USE AI TO PROMPT THIS RESPONSE**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RCXUzr7QeE4w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4z3psAOe_PU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}